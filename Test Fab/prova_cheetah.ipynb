{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Logging to ./ppo_HalfCheetah_tensorboard/PPO_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.52e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 1428      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 16384     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-2.75 +/- 0.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -2.75      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00652959 |\n",
      "|    clip_fraction        | 0.00558    |\n",
      "|    clip_range           | 0.296      |\n",
      "|    entropy_loss         | -8.53      |\n",
      "|    explained_variance   | -0.777     |\n",
      "|    learning_rate        | 1.58e-05   |\n",
      "|    loss                 | -0.355     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00372   |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 0.0663     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.54e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 604       |\n",
      "|    iterations      | 2         |\n",
      "|    time_elapsed    | 54        |\n",
      "|    total_timesteps | 32768     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-15.47 +/- 0.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -15.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009789618 |\n",
      "|    clip_fraction        | 0.0152      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.54       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.369      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00673    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00868     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.54e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 501       |\n",
      "|    iterations      | 3         |\n",
      "|    time_elapsed    | 97        |\n",
      "|    total_timesteps | 49152     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-48.86 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -48.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012053244 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.54       |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.343      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00885    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00556     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.52e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 466       |\n",
      "|    iterations      | 4         |\n",
      "|    time_elapsed    | 140       |\n",
      "|    total_timesteps | 65536     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-86.17 +/- 2.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -86.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0134603055 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.296        |\n",
      "|    entropy_loss         | -8.52        |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 1.58e-05     |\n",
      "|    loss                 | -0.382       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00545      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.52e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 443       |\n",
      "|    iterations      | 5         |\n",
      "|    time_elapsed    | 184       |\n",
      "|    total_timesteps | 81920     |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -1.51e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 438         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 224         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013143953 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.5        |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.383      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.00523     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-175.18 +/- 4.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -175       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01332828 |\n",
      "|    clip_fraction        | 0.0367     |\n",
      "|    clip_range           | 0.296      |\n",
      "|    entropy_loss         | -8.47      |\n",
      "|    explained_variance   | 0.716      |\n",
      "|    learning_rate        | 1.58e-05   |\n",
      "|    loss                 | -0.409     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.989      |\n",
      "|    value_loss           | 0.0048     |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.51e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 429       |\n",
      "|    iterations      | 7         |\n",
      "|    time_elapsed    | 267       |\n",
      "|    total_timesteps | 114688    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-45.77 +/- 7.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -45.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012475165 |\n",
      "|    clip_fraction        | 0.0331      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.44       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.387      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00909    |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 0.00483     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -1.5e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 423      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 309      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-13.11 +/- 13.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -13.1      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 140000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01358328 |\n",
      "|    clip_fraction        | 0.0447     |\n",
      "|    clip_range           | 0.296      |\n",
      "|    entropy_loss         | -8.41      |\n",
      "|    explained_variance   | 0.767      |\n",
      "|    learning_rate        | 1.58e-05   |\n",
      "|    loss                 | -0.375     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.982      |\n",
      "|    value_loss           | 0.00516    |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.48e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 418       |\n",
      "|    iterations      | 9         |\n",
      "|    time_elapsed    | 352       |\n",
      "|    total_timesteps | 147456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-114.10 +/- 35.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -114       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01431725 |\n",
      "|    clip_fraction        | 0.0466     |\n",
      "|    clip_range           | 0.296      |\n",
      "|    entropy_loss         | -8.39      |\n",
      "|    explained_variance   | 0.678      |\n",
      "|    learning_rate        | 1.58e-05   |\n",
      "|    loss                 | -0.343     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.976      |\n",
      "|    value_loss           | 0.00436    |\n",
      "----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.47e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 414       |\n",
      "|    iterations      | 10        |\n",
      "|    time_elapsed    | 395       |\n",
      "|    total_timesteps | 163840    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-191.63 +/- 12.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -192        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012349695 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.35       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.378      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.45e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 411       |\n",
      "|    iterations      | 11        |\n",
      "|    time_elapsed    | 437       |\n",
      "|    total_timesteps | 180224    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -1.44e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 411         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 477         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011506294 |\n",
      "|    clip_fraction        | 0.0331      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.32       |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.388      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00961    |\n",
      "|    std                  | 0.965       |\n",
      "|    value_loss           | 0.00468     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-152.71 +/- 22.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -153         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112654865 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.296        |\n",
      "|    entropy_loss         | -8.28        |\n",
      "|    explained_variance   | 0.665        |\n",
      "|    learning_rate        | 1.58e-05     |\n",
      "|    loss                 | -0.37        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00887     |\n",
      "|    std                  | 0.958        |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.43e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 410       |\n",
      "|    iterations      | 13        |\n",
      "|    time_elapsed    | 519       |\n",
      "|    total_timesteps | 212992    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-124.77 +/- 15.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -125        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011917574 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.382      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00804    |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 0.00508     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.42e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 408       |\n",
      "|    iterations      | 14        |\n",
      "|    time_elapsed    | 561       |\n",
      "|    total_timesteps | 229376    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-17.52 +/- 32.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -17.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012065436 |\n",
      "|    clip_fraction        | 0.0368      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.22       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.346      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.949       |\n",
      "|    value_loss           | 0.00389     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.41e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 406       |\n",
      "|    iterations      | 15        |\n",
      "|    time_elapsed    | 604       |\n",
      "|    total_timesteps | 245760    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-60.63 +/- 65.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -60.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011480579 |\n",
      "|    clip_fraction        | 0.0352      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.18       |\n",
      "|    explained_variance   | 0.639       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.335      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.943       |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -1.4e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 405      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 646      |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -1.39e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 406         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 685         |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011366235 |\n",
      "|    clip_fraction        | 0.0347      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.15       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.359      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.939       |\n",
      "|    value_loss           | 0.00403     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=0.97 +/- 53.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 0.971       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012399197 |\n",
      "|    clip_fraction        | 0.0371      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.11       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.355      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 0.004       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.38e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 405       |\n",
      "|    iterations      | 18        |\n",
      "|    time_elapsed    | 727       |\n",
      "|    total_timesteps | 294912    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=41.12 +/- 26.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 41.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011616677 |\n",
      "|    clip_fraction        | 0.0348      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.06       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.33       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    std                  | 0.925       |\n",
      "|    value_loss           | 0.00413     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.36e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 404       |\n",
      "|    iterations      | 19        |\n",
      "|    time_elapsed    | 769       |\n",
      "|    total_timesteps | 311296    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=40.42 +/- 19.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 40.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011241341 |\n",
      "|    clip_fraction        | 0.0314      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -8.02       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.362      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.918       |\n",
      "|    value_loss           | 0.00412     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.34e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 403       |\n",
      "|    iterations      | 20        |\n",
      "|    time_elapsed    | 812       |\n",
      "|    total_timesteps | 327680    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=23.22 +/- 20.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 23.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011450384 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.97       |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.323      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 0.00429     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.32e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 402       |\n",
      "|    iterations      | 21        |\n",
      "|    time_elapsed    | 854       |\n",
      "|    total_timesteps | 344064    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=94.37 +/- 30.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 94.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 360000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012102809 |\n",
      "|    clip_fraction        | 0.0369      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.92       |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.369      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.903       |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -1.3e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 401      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 897      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -1.28e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 402         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 936         |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012352752 |\n",
      "|    clip_fraction        | 0.0352      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.88       |\n",
      "|    explained_variance   | 0.553       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.379      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    std                  | 0.895       |\n",
      "|    value_loss           | 0.00458     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=69.75 +/- 27.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 69.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010712015 |\n",
      "|    clip_fraction        | 0.0288      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.82       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.342      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.888       |\n",
      "|    value_loss           | 0.00456     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.26e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 401       |\n",
      "|    iterations      | 24        |\n",
      "|    time_elapsed    | 978       |\n",
      "|    total_timesteps | 393216    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=57.70 +/- 144.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 57.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012453687 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.78       |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.338      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.882       |\n",
      "|    value_loss           | 0.00465     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.24e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 401       |\n",
      "|    iterations      | 25        |\n",
      "|    time_elapsed    | 1020      |\n",
      "|    total_timesteps | 409600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-105.15 +/- 225.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -105         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0120807225 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.296        |\n",
      "|    entropy_loss         | -7.73        |\n",
      "|    explained_variance   | 0.722        |\n",
      "|    learning_rate        | 1.58e-05     |\n",
      "|    loss                 | -0.356       |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0136      |\n",
      "|    std                  | 0.874        |\n",
      "|    value_loss           | 0.00426      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 400       |\n",
      "|    iterations      | 26        |\n",
      "|    time_elapsed    | 1063      |\n",
      "|    total_timesteps | 425984    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=177.22 +/- 34.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 177         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011082715 |\n",
      "|    clip_fraction        | 0.0288      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.68       |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.361      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.867       |\n",
      "|    value_loss           | 0.00436     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.18e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 400       |\n",
      "|    iterations      | 27        |\n",
      "|    time_elapsed    | 1105      |\n",
      "|    total_timesteps | 442368    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -1.15e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 400         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1145        |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012921603 |\n",
      "|    clip_fraction        | 0.04        |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.62       |\n",
      "|    explained_variance   | 0.605       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.371      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    std                  | 0.858       |\n",
      "|    value_loss           | 0.00444     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=151.10 +/- 71.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 151          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 460000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113965105 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.296        |\n",
      "|    entropy_loss         | -7.56        |\n",
      "|    explained_variance   | 0.606        |\n",
      "|    learning_rate        | 1.58e-05     |\n",
      "|    loss                 | -0.356       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    std                  | 0.848        |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.13e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 400       |\n",
      "|    iterations      | 29        |\n",
      "|    time_elapsed    | 1187      |\n",
      "|    total_timesteps | 475136    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=129.81 +/- 139.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 130         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011561707 |\n",
      "|    clip_fraction        | 0.0378      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.5        |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.362      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.842       |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -1.1e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 399      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 1229     |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-4.56 +/- 123.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -4.56       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011277527 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.45       |\n",
      "|    explained_variance   | 0.634       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.337      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.835       |\n",
      "|    value_loss           | 0.00443     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.08e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 399       |\n",
      "|    iterations      | 31        |\n",
      "|    time_elapsed    | 1271      |\n",
      "|    total_timesteps | 507904    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-24.10 +/- 0.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -24.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010494397 |\n",
      "|    clip_fraction        | 0.0291      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.4        |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.333      |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.828       |\n",
      "|    value_loss           | 0.00449     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.05e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 399       |\n",
      "|    iterations      | 32        |\n",
      "|    time_elapsed    | 1313      |\n",
      "|    total_timesteps | 524288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-21.31 +/- 0.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -21.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011570394 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.35       |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.346      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.821       |\n",
      "|    value_loss           | 0.00468     |\n",
      "-----------------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -1.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 398       |\n",
      "|    iterations      | 33        |\n",
      "|    time_elapsed    | 1355      |\n",
      "|    total_timesteps | 540672    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -1.01e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 399         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 1394        |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011666305 |\n",
      "|    clip_fraction        | 0.0346      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.3        |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.321      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    std                  | 0.814       |\n",
      "|    value_loss           | 0.00483     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-122.95 +/- 2.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -123         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 560000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0122922845 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.296        |\n",
      "|    entropy_loss         | -7.24        |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 1.58e-05     |\n",
      "|    loss                 | -0.364       |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.0145      |\n",
      "|    std                  | 0.806        |\n",
      "|    value_loss           | 0.0045       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -986     |\n",
      "| time/              |          |\n",
      "|    fps             | 399      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 1435     |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=309.68 +/- 33.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 310         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011599971 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.18       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.311      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    std                  | 0.798       |\n",
      "|    value_loss           | 0.00462     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -957     |\n",
      "| time/              |          |\n",
      "|    fps             | 398      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 1478     |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=284.37 +/- 22.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 284         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011261065 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.318      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    std                  | 0.79        |\n",
      "|    value_loss           | 0.0045      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -932     |\n",
      "| time/              |          |\n",
      "|    fps             | 398      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 1520     |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=316.93 +/- 15.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 317         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 620000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011869432 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7.06       |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.289      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    std                  | 0.782       |\n",
      "|    value_loss           | 0.00447     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -911     |\n",
      "| time/              |          |\n",
      "|    fps             | 398      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 1563     |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -891        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 398         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 1602        |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012398282 |\n",
      "|    clip_fraction        | 0.0416      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -7          |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.319      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    std                  | 0.774       |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=357.44 +/- 23.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 357         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010777604 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -6.95       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.341      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    std                  | 0.769       |\n",
      "|    value_loss           | 0.00432     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -871     |\n",
      "| time/              |          |\n",
      "|    fps             | 398      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 1645     |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=346.41 +/- 33.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 346          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 660000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0120130535 |\n",
      "|    clip_fraction        | 0.0426       |\n",
      "|    clip_range           | 0.296        |\n",
      "|    entropy_loss         | -6.91        |\n",
      "|    explained_variance   | 0.714        |\n",
      "|    learning_rate        | 1.58e-05     |\n",
      "|    loss                 | -0.311       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    std                  | 0.763        |\n",
      "|    value_loss           | 0.00464      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -847     |\n",
      "| time/              |          |\n",
      "|    fps             | 397      |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 1689     |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=319.18 +/- 18.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 319         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 680000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011597032 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -6.86       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.323      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    std                  | 0.756       |\n",
      "|    value_loss           | 0.00396     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -815     |\n",
      "| time/              |          |\n",
      "|    fps             | 396      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 1734     |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=364.34 +/- 11.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 364        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 700000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00985574 |\n",
      "|    clip_fraction        | 0.0267     |\n",
      "|    clip_range           | 0.296      |\n",
      "|    entropy_loss         | -6.8       |\n",
      "|    explained_variance   | 0.696      |\n",
      "|    learning_rate        | 1.58e-05   |\n",
      "|    loss                 | -0.305     |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.749      |\n",
      "|    value_loss           | 0.00382    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -790     |\n",
      "| time/              |          |\n",
      "|    fps             | 396      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 1778     |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=218.92 +/- 271.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 219         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010808913 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -6.73       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.283      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.74        |\n",
      "|    value_loss           | 0.00363     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -771     |\n",
      "| time/              |          |\n",
      "|    fps             | 395      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 1820     |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -742        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 1860        |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012319618 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -6.67       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.301      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    std                  | 0.733       |\n",
      "|    value_loss           | 0.00399     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=401.30 +/- 18.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 401        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 740000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01283651 |\n",
      "|    clip_fraction        | 0.0406     |\n",
      "|    clip_range           | 0.296      |\n",
      "|    entropy_loss         | -6.61      |\n",
      "|    explained_variance   | 0.692      |\n",
      "|    learning_rate        | 1.58e-05   |\n",
      "|    loss                 | -0.318     |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.725      |\n",
      "|    value_loss           | 0.00343    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -716     |\n",
      "| time/              |          |\n",
      "|    fps             | 395      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 1903     |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=254.54 +/- 335.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 255        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 760000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01302259 |\n",
      "|    clip_fraction        | 0.0455     |\n",
      "|    clip_range           | 0.296      |\n",
      "|    entropy_loss         | -6.54      |\n",
      "|    explained_variance   | 0.735      |\n",
      "|    learning_rate        | 1.58e-05   |\n",
      "|    loss                 | -0.298     |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.718      |\n",
      "|    value_loss           | 0.00344    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -697     |\n",
      "| time/              |          |\n",
      "|    fps             | 395      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 1946     |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=409.11 +/- 16.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 409         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012163889 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -6.48       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.269      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 0.00332     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -682     |\n",
      "| time/              |          |\n",
      "|    fps             | 395      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 1989     |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=50.34 +/- 435.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 50.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011635083 |\n",
      "|    clip_fraction        | 0.0353      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.296      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -662     |\n",
      "| time/              |          |\n",
      "|    fps             | 395      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 2031     |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -636         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 395          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 2070         |\n",
      "|    total_timesteps      | 819200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0123535395 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.296        |\n",
      "|    entropy_loss         | -6.35        |\n",
      "|    explained_variance   | 0.679        |\n",
      "|    learning_rate        | 1.58e-05     |\n",
      "|    loss                 | -0.307       |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.0148      |\n",
      "|    std                  | 0.694        |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=408.07 +/- 30.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 408         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012054288 |\n",
      "|    clip_fraction        | 0.0327      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.311      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    std                  | 0.686       |\n",
      "|    value_loss           | 0.00313     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -613     |\n",
      "| time/              |          |\n",
      "|    fps             | 395      |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 2112     |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=426.54 +/- 13.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 427         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012679616 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -6.2        |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.262      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    std                  | 0.678       |\n",
      "|    value_loss           | 0.00308     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -593     |\n",
      "| time/              |          |\n",
      "|    fps             | 395      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 2155     |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=447.88 +/- 9.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 448          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 860000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0137774125 |\n",
      "|    clip_fraction        | 0.0476       |\n",
      "|    clip_range           | 0.296        |\n",
      "|    entropy_loss         | -6.14        |\n",
      "|    explained_variance   | 0.821        |\n",
      "|    learning_rate        | 1.58e-05     |\n",
      "|    loss                 | -0.278       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    std                  | 0.672        |\n",
      "|    value_loss           | 0.00303      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -568     |\n",
      "| time/              |          |\n",
      "|    fps             | 395      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 2198     |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=438.70 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 439         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011720356 |\n",
      "|    clip_fraction        | 0.036       |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -6.08       |\n",
      "|    explained_variance   | 0.719       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.291      |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    std                  | 0.665       |\n",
      "|    value_loss           | 0.00279     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -547     |\n",
      "| time/              |          |\n",
      "|    fps             | 394      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 2240     |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=445.84 +/- 14.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 446         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010963986 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.252      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.657       |\n",
      "|    value_loss           | 0.00276     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -526     |\n",
      "| time/              |          |\n",
      "|    fps             | 394      |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 2284     |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -508        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 394         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 2324        |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012460403 |\n",
      "|    clip_fraction        | 0.036       |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -5.94       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.225      |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    std                  | 0.649       |\n",
      "|    value_loss           | 0.00267     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=462.47 +/- 24.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 462         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012224633 |\n",
      "|    clip_fraction        | 0.0384      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -5.87       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.26       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    std                  | 0.641       |\n",
      "|    value_loss           | 0.00272     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -489     |\n",
      "| time/              |          |\n",
      "|    fps             | 394      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 2365     |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=465.09 +/- 22.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 465          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 940000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0119467415 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.296        |\n",
      "|    entropy_loss         | -5.79        |\n",
      "|    explained_variance   | 0.715        |\n",
      "|    learning_rate        | 1.58e-05     |\n",
      "|    loss                 | -0.235       |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.0145      |\n",
      "|    std                  | 0.633        |\n",
      "|    value_loss           | 0.00247      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -467     |\n",
      "| time/              |          |\n",
      "|    fps             | 394      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 2408     |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=488.16 +/- 21.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 488         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013694089 |\n",
      "|    clip_fraction        | 0.0499      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -5.72       |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.32       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    std                  | 0.626       |\n",
      "|    value_loss           | 0.00238     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -444     |\n",
      "| time/              |          |\n",
      "|    fps             | 394      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 2452     |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=487.48 +/- 16.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 487         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 980000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012340931 |\n",
      "|    clip_fraction        | 0.0445      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.285      |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    std                  | 0.618       |\n",
      "|    value_loss           | 0.00243     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -421     |\n",
      "| time/              |          |\n",
      "|    fps             | 393      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 2496     |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -404        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 2536        |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013007423 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -5.57       |\n",
      "|    explained_variance   | 0.685       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.272      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    std                  | 0.611       |\n",
      "|    value_loss           | 0.00232     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=509.39 +/- 22.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 509         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013481205 |\n",
      "|    clip_fraction        | 0.0483      |\n",
      "|    clip_range           | 0.296       |\n",
      "|    entropy_loss         | -5.5        |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 1.58e-05    |\n",
      "|    loss                 | -0.261      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    std                  | 0.604       |\n",
      "|    value_loss           | 0.0023      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -381     |\n",
      "| time/              |          |\n",
      "|    fps             | 393      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 2578     |\n",
      "|    total_timesteps | 1015808  |\n",
      "---------------------------------\n",
      "Mean Reward: 509.83  21.38\n"
     ]
    }
   ],
   "source": [
    "# Usa SubprocVecEnv per sfruttare il multiprocessing (pi veloce di DummyVecEnv)\n",
    "NUM_ENVS = 4  # Numero di ambienti paralleli per accelerare il training\n",
    "\n",
    "# Definiamo la funzione per creare un ambiente vettorializzato\n",
    "def make_env():\n",
    "    return Monitor(gym.make(\"HalfCheetah-v5\",\n",
    "                            reset_noise_scale=0.0719410443033492,\n",
    "                            forward_reward_weight=0.8079894174326131,\n",
    "                            ctrl_cost_weight=0.47961956759514446,\n",
    "                            render_mode='none'))\n",
    "\n",
    "# Creiamo gli ambienti paralleli\n",
    "env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "# Parametri del modello (puoi ottimizzarli con Optuna)\n",
    "model_params = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env,\n",
    "    \"learning_rate\": 1.582554022730496e-05,  # Valore tipico per HalfCheetah\n",
    "    \"n_steps\": 4096,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 10,\n",
    "    \"gamma\": 0.9895868337827359,\n",
    "    \"gae_lambda\": 0.8258089470360688,\n",
    "    \"clip_range\": 0.2961352072414478,\n",
    "    \"ent_coef\": 0.043318253089964606,\n",
    "    \"verbose\": 1,\n",
    "    \"tensorboard_log\": \"./ppo_HalfCheetah_tensorboard/\",\n",
    "    \"device\": \"mps\",  # Usa GPU se disponibile\n",
    "    \"policy_kwargs\": dict(net_arch=[256, 256, 128])\n",
    "}\n",
    "\n",
    "# Definiamo i callback per salvataggio e valutazione\n",
    "eval_env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/best_model\",\n",
    "                             log_path=\"./logs/\", eval_freq=5000, deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=\"./logs/checkpoints/\",\n",
    "                                         name_prefix=\"ppo_halfcheetah_checkpoint\")\n",
    "\n",
    "# Training del modello\n",
    "model = PPO(**model_params)\n",
    "model.learn(total_timesteps=1_000_000, callback=CallbackList([eval_callback, checkpoint_callback]))\n",
    "\n",
    "# Salvataggio del modello e della normalizzazione\n",
    "model.save(\"ppo_HalfCheetah_model\")\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")\n",
    "\n",
    "# Funzione di valutazione migliorata\n",
    "def evaluate_agent(model, env, episodes=100):\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=episodes, deterministic=True)\n",
    "    print(f\"Mean Reward: {mean_reward:.2f}  {std_reward:.2f}\")\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# Valutiamo il modello addestrato\n",
    "mean_reward_trained, std_reward_trained = evaluate_agent(model, env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 22:56:18.810 Python[18346:583589] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-13 22:56:18.810 Python[18346:583589] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:234: UserWarning: Starting from gymnasium v0.26, render modes are determined during the initialization of the environment.\n",
      "                We allow to pass a mode argument to maintain a backwards compatible VecEnv API, but the mode (rgb_array)\n",
      "                has to be the same as the environment render mode (human) which is not the case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The image must have at least two spatial dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Registra un video della policy ottimale\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mrender_and_record_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mppo_HalfCheetah_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 44\u001b[0m, in \u001b[0;36mrender_and_record_policy\u001b[0;34m(model_path, output_filename, episodes)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     42\u001b[0m         obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 44\u001b[0m \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmimsave\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo salvato in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/imageio/v2.py:495\u001b[0m, in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/imageio/core/legacy_plugin_wrapper.py:242\u001b[0m, in \u001b[0;36mLegacyPlugin.write\u001b[0;34m(self, ndimage, is_batch, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(image)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe image must have at least two spatial dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(image\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mnumber) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\n\u001b[1;32m    247\u001b[0m     image\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    248\u001b[0m ):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll images have to be numeric, and not `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The image must have at least two spatial dimensions."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "NUM_ENVS=2\n",
    "def make_envv():\n",
    "    return Monitor(gym.make(\"HalfCheetah-v5\",\n",
    "                            reset_noise_scale=0.0719410443033492,\n",
    "                            forward_reward_weight=0.8079894174326131,\n",
    "                            ctrl_cost_weight=0.47961956759514446,\n",
    "                            render_mode='human'))\n",
    "\n",
    "# Creiamo gli ambienti paralleli\n",
    "env = SubprocVecEnv([make_envv for _ in range(NUM_ENVS)])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "# Funzione per visualizzare la policy in tempo reale e registrare il video\n",
    "def render_and_record_policy(model_path, output_filename=\"videos/halfcheetah_best_policy.mp4\", episodes=1):\n",
    "    os.makedirs(\"videos\", exist_ok=True)\n",
    "    env = make_envv()\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize.load(\"vecnormalize_HalfCheetah.pkl\", env)\n",
    "    env.training = False  # Disabilita la normalizzazione della reward per la valutazione\n",
    "    env.norm_reward = False\n",
    "    \n",
    "    model = PPO.load(model_path, env=env)\n",
    "    obs = env.reset()\n",
    "    frames = []\n",
    "    \n",
    "    for _ in range(episodes * 1000):  # Esegui abbastanza step per registrare un episodio completo\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, _, done, _ = env.step(action)\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "    \n",
    "    imageio.mimsave(output_filename, frames, fps=30)\n",
    "    print(f\"Video salvato in {output_filename}\")\n",
    "    env.close()\n",
    "\n",
    "# Registra un video della policy ottimale\n",
    "render_and_record_policy(\"ppo_HalfCheetah_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
