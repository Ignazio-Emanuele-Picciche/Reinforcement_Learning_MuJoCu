{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Training completo tra 5mln a 10mln di TimeStamp e tra 5000 e 10000 episodi\n",
    "\n",
    "-Per un tuning rapido da 500k a 1mln di TimeStamp e tra 500 a 1k episodi per trial (consigliati 500 trial)\n",
    "\n",
    "-Per i test preliminari 1mln di timestamp e 1k/2k episodi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHAT con search dice che per il train vanno bene anche 1mln di timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAC_1 -> [I 2025-02-16 08:35:02,806] Trial 6 finished with value: 2336.717724201964 and parameters: {'reset_noise_scale': 0.02535169706650553, 'forward_reward_weight': 1.6947930083902139, 'ctrl_cost_weight': 1.3382414657833144, 'healthy_reward': 2.454361643694453, 'contact_cost_weight': 9.801460983142638e-05, 'healthy_z_lower': 0.26138660668179525, 'healthy_z_upper': 1.2798527262125532, 'contact_force_min': -1.2175000215851723, 'contact_force_max': 0.8378346397233374, 'learning_rate': 0.0001093423040329136, 'batch_size': 256, 'gamma': 0.9586862210519205, 'tau': 0.03821261407257494, 'ent_coef': 0.01806334596909146, 'train_freq': 2, 'gradient_steps': 6}. Best is trial 6 with value: 2336.717724201964.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I 2025-02-17 15:01:06,047] Trial 131 finished with value: 2877.213708915524 and parameters: {'reset_noise_scale': 0.18076079550448898, 'forward_reward_weight': 1.6236656710889608, 'ctrl_cost_weight': 1.2650706815726114, 'healthy_reward': 2.40623966986497, 'contact_cost_weight': 9.78401407249111e-06, 'healthy_z_lower': 0.17310762633784954, 'healthy_z_upper': 1.3741885293867462, 'contact_force_min': -1.2191856901258815, 'contact_force_max': 0.8954759302944116, 'learning_rate': 0.0008510732994554318, 'learning_starts': 10000, 'batch_size': 256, 'gamma': 0.9520212353011819, 'tau': 0.024539835196007352, 'ent_coef': 0.0139500037156449, 'train_freq': 3, 'gradient_steps': 6}. Best is trial 131 with value: 2877.213708915524."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAC_2 -> [I 2025-02-17 18:33:32,594] Trial 234 finished with value: 2921.6025903890572 and parameters: {'reset_noise_scale': 0.20141630762026747, 'forward_reward_weight': 1.6278491287471968, 'ctrl_cost_weight': 1.2763028508139123, 'healthy_reward': 2.399103414226093, 'contact_cost_weight': 5.2247028604018e-06, 'healthy_z_lower': 0.16940418341206293, 'healthy_z_upper': 1.3272275689129294, 'contact_force_min': -1.2110815310071597, 'contact_force_max': 0.8982646857233315, 'learning_rate': 0.0006839314025125863, 'learning_starts': 4000, 'batch_size': 512, 'gamma': 0.9508951442412019, 'tau': 0.019742481119711143, 'ent_coef': 0.012500206557457209, 'train_freq': 3, 'gradient_steps': 7}. Best is trial 234 with value: 2921.6025903890572.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I 2025-02-17 19:58:58,563] Trial 266 finished with value: 2606.9031733902284 and parameters: {'reset_noise_scale': 0.19226821652309825, 'forward_reward_weight': 1.6252187818921442, 'ctrl_cost_weight': 1.3518269064593467, 'healthy_reward': 2.404837557170063, 'contact_cost_weight': 7.933389808519208e-05, 'healthy_z_lower': 0.1683208288680806, 'healthy_z_upper': 1.3528414909566904, 'contact_force_min': -1.2353928919135455, 'contact_force_max': 0.888390169403758, 'learning_rate': 0.0008018300610795172, 'learning_starts': 10000, 'batch_size': 512, 'gamma': 0.9505557781892291, 'tau': 0.021876855067651642, 'ent_coef': 0.04252279860783029, 'train_freq': 3, 'gradient_steps': 6}. Best is trial 234 with value: 2921.6025903890572.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipreparametri dell'envrionment\n",
    "hp_reset_noise_scale= 0.20141630762026747 # scala del rumore quando l'ambiente viene resettato \n",
    "hp_forward_reward_weight = 1.6278491287471968 # peso del reward per il movimento in avanti\n",
    "hp_ctrl_cost_weight = 1.2763028508139123 # peso del reward per il controllo\n",
    "hp_healthy_reward = 2.399103414226093 # reward per la salute\n",
    "\n",
    "hp_contact_cost_weight = 5.2247028604018e-06\n",
    "healthy_z = (0.16940418341206293, 1.3272275689129294)\n",
    "contact_force = (-1.2110815310071597, 0.8982646857233315)\n",
    "\n",
    "\n",
    "# Iperparametri del modello/policy\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "hp_learning_rate=0.0006839314025125863          # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "hp_learning_starts=4000           # Numero di passi di esplorazione prima di iniziare l'apprendimento\n",
    "hp_batch_size=512                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "hp_gamma=0.9508951442412019      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "hp_tau=0.019742481119711143      # Tasso di aggiornamento del target: controlla la velocità con cui i pesi del target network vengono aggiornati\n",
    "hp_ent_coef=0.012500206557457209                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n",
    "hp_train_freq=3             # Frequenza di aggiornamento della policy: controlla la frequenza con cui la policy viene aggiornata\n",
    "hp_gradient_steps=7         # Numero di passaggi di ottimizzazione per aggiornare la policy: controlla la quantità di aggiornamenti della policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrà effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=hp_reset_noise_scale, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=hp_forward_reward_weight, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=hp_ctrl_cost_weight, # peso del reward per il controllo\n",
    "                    healthy_reward =hp_healthy_reward, # reward per la salute\n",
    "                    contact_cost_weight=hp_contact_cost_weight,\n",
    "                    healthy_z_range=healthy_z,\n",
    "                    contact_force_range=contact_force,\n",
    "                    render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire più istanze dell'ambiente come se fossero una singola entità.\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "#env = DummyVecEnv([make_env])  \n",
    "\n",
    "\n",
    "NUM_ENVS=8\n",
    "env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def linear_schedule(initial_value):\n",
    "#     def func(progress_remaining):\n",
    "#         return progress_remaining * initial_value\n",
    "#     return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea ed allena il modello SAC con i parametri ottimizzati\n",
    "model = SAC(\"MlpPolicy\", env,\n",
    "            learning_rate=hp_learning_rate,\n",
    "            learning_starts=hp_learning_starts,\n",
    "            buffer_size=50000,\n",
    "            batch_size=hp_batch_size,\n",
    "            gamma=hp_gamma,\n",
    "            tau=hp_tau,\n",
    "            ent_coef=hp_ent_coef,\n",
    "            train_freq=hp_train_freq,\n",
    "            gradient_steps=hp_gradient_steps,\n",
    "            # seed=42,\n",
    "            verbose=0,\n",
    "            tensorboard_log=\"./sac_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "            device='mps'                    # Specifica l'uso della GPU su Apple Silicon\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#eval_env = DummyVecEnv([make_env])\n",
    "\n",
    "eval_env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/best_model\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=50000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=400000, episode_reward=47.76 +/- 24.22\n",
      "Episode length: 801.60 +/- 396.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=800000, episode_reward=69.76 +/- 2.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1200000, episode_reward=74.46 +/- 2.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1600000, episode_reward=70.76 +/- 10.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=76.01 +/- 2.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x3718eb160>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = 2000000  # Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\n",
    "model.learn(total_timesteps=total_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"sac_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200-400 episodi sono adeguati \n",
    "# def evaluate_policy(env, policy, episodes=500):\n",
    "#     \"\"\"\n",
    "#     Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "#     Parametri:\n",
    "#     - env: L'ambiente di simulazione.\n",
    "#     - policy: La policy addestrata da valutare.\n",
    "#     - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "#     Ritorna:\n",
    "#     - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "#     \"\"\"\n",
    "#     total_rewards = []\n",
    "#     for _ in range(episodes):\n",
    "#         obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "#         while not done:\n",
    "#             action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "#             obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "#             total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "#         total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "#     return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_random_policy(env, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = [False] * env.num_envs\n",
    "        episode_rewards = np.zeros(env.num_envs)\n",
    "        while not all(done):\n",
    "            actions = [env.action_space.sample() for _ in range(env.num_envs)]\n",
    "            obs, rewards, done, infos = env.step(actions)\n",
    "            episode_rewards += rewards\n",
    "        total_rewards.extend(episode_rewards)\n",
    "    mean_reward_random = np.mean(total_rewards)\n",
    "    # std_reward_random = np.std(total_rewards)\n",
    "    return mean_reward_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.training = False # Setta l'environment in modalità di valutazione\n",
    "# env.norm_reward = False # Disabilita la normalizzazione della reward. Questo è importante per valutare correttamente il modello.\n",
    "\n",
    "# # Valutazione dopo l'addestramento\n",
    "# mean_reward_trained, std_reward_trained = evaluate_policy(model, env, n_eval_episodes=500)  # Valuta la policy addestrata\n",
    "# mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# # Stampa dei risultati\n",
    "# print(f\"Trained Policy: Mean Reward: {mean_reward_trained}\")\n",
    "# print(f\"Random Policy: Mean Reward: {mean_reward_random}\")\n",
    "\n",
    "# # Creazione del grafico di confronto\n",
    "# # labels = ['Random Policy', 'Trained Policy']\n",
    "# # means = [mean_reward_random, mean_reward_trained]\n",
    "# # stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "# # plt.figure(figsize=(8, 5))\n",
    "# # plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "# # plt.ylabel('Mean Episodic Reward')\n",
    "# # plt.title('Policy Comparison')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
