{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/glfw/__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Caricamento del modello PPO addestrato\n",
    "# Assicurati che il percorso al file sia corretto e che il modello esista\n",
    "model = PPO.load(\"ppo_HalfCheetah_model\")\n",
    "\n",
    "# Creazione dell'ambiente in modalità vettoriale con DummyVecEnv\n",
    "# Questo consente la compatibilità con le tecniche di normalizzazione come VecNormalize\n",
    "render_env = DummyVecEnv([lambda: gym.make(\"HalfCheetah-v5\",\n",
    "                                           reset_noise_scale=0.013459312664159742,  # Intensità del rumore alla reset\n",
    "                                           forward_reward_weight=1.4435374113892951,  # Peso della ricompensa per il movimento in avanti\n",
    "                                           ctrl_cost_weight=0.09129087622076545,  # Peso del costo del controllo (azione)\n",
    "                                           render_mode='human')])  # Modalità di rendering (può essere 'human' o 'rgb_array')\n",
    "\n",
    "# Caricamento della normalizzazione dello stato e delle ricompense\n",
    "# Assicurati che il percorso al file sia corretto e che il file di normalizzazione esista\n",
    "render_env = VecNormalize.load(\"vecnormalize_HalfCheetah.pkl\", render_env)\n",
    "\n",
    "# Disabilitiamo l'aggiornamento delle statistiche per la normalizzazione, utile in fase di test\n",
    "render_env.training = False\n",
    "render_env.norm_reward = True  # Se True, normalizza anche le ricompense\n",
    "\n",
    "# Reset dell'ambiente per ottenere l'osservazione iniziale (array numpy)\n",
    "obs = render_env.reset()\n",
    "\n",
    "done = False  # Variabile che indica se l'episodio è terminato\n",
    "\n",
    "# Loop per eseguire la simulazione e il rendering dell'ambiente\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)  # Predizione dell'azione ottimale con il modello PPO\n",
    "    obs, rewards, dones, infos = render_env.step(action)  # Applichiamo l'azione e otteniamo i nuovi dati dall'ambiente\n",
    "    render_env.render()  # Visualizza l'ambiente a schermo (solo se render_mode='human')\n",
    "    time.sleep(0.01)  # Piccola pausa per rallentare il rendering e renderlo visibile\n",
    "\n",
    "    # Gestione del termine dell'episodio\n",
    "    # `dones` è un array (per compatibilità con più ambienti), prendiamo il primo valore\n",
    "    done = dones[0]\n",
    "\n",
    "# Chiusura dell'ambiente per rilasciare le risorse\n",
    "render_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 16:18:51.161 Python[32697:6082046] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-04 16:18:51.161 Python[32697:6082046] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Carichiamo il modello PPO addestrato\n",
    "# Assicurati che il percorso del file sia corretto\n",
    "model = PPO.load(\"ppo_HalfCheetah_model\")\n",
    "\n",
    "# Funzione per creare l'ambiente personalizzato con i parametri specificati\n",
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente \"HalfCheetah-v5\" dalla libreria Gym con i parametri specificati.\n",
    "\n",
    "    Questo ambiente viene configurato con valori personalizzati per:\n",
    "    - reset_noise_scale: livello di rumore al momento del reset dell'ambiente.\n",
    "    - forward_reward_weight: peso della ricompensa per il movimento in avanti.\n",
    "    - ctrl_cost_weight: peso del costo del controllo (energia utilizzata per il movimento).\n",
    "    - render_mode: impostato su 'rgb_array' per ottenere i frame in formato immagine.\n",
    "\n",
    "    Returns:\n",
    "        gym.Env: Un'istanza dell'ambiente \"HalfCheetah-v5\" configurata con i parametri specificati.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"HalfCheetah-v5\",\n",
    "                   reset_noise_scale=0.013459312664159742,  # Intensità del rumore alla reset\n",
    "                   forward_reward_weight=1.4435374113892951,  # Peso della ricompensa per il movimento in avanti\n",
    "                   ctrl_cost_weight=0.09129087622076545,  # Peso del costo del controllo (azione)\n",
    "                   render_mode='rgb_array')  # Usa 'rgb_array' per ottenere i frame in formato immagine\n",
    "    return env\n",
    "\n",
    "# Creiamo un DummyVecEnv per avvolgere l'ambiente, necessario per l'uso di VecNormalize\n",
    "render_env = DummyVecEnv([make_env])\n",
    "\n",
    "# Carichiamo la normalizzazione salvata dallo stesso file usato durante l'addestramento\n",
    "# Assicurati che il percorso al file sia corretto\n",
    "render_env = VecNormalize.load(\"vecnormalize_HalfCheetah.pkl\", render_env)\n",
    "\n",
    "# Disattiviamo l'aggiornamento delle statistiche per la normalizzazione (utile in fase di valutazione)\n",
    "render_env.training = False\n",
    "render_env.norm_reward = True  # Se True, continua a normalizzare le ricompense\n",
    "\n",
    "# Reset dell'ambiente per ottenere l'osservazione iniziale (array numpy)\n",
    "obs = render_env.reset()\n",
    "done = False  # Variabile che indica se l'episodio è terminato\n",
    "\n",
    "# Loop per il rendering e la simulazione dell'agente\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)  # Predizione dell'azione ottimale con il modello PPO\n",
    "    obs, reward, done, info = render_env.step(action)  # Esegui l'azione e ottieni i nuovi dati dall'ambiente\n",
    "\n",
    "    # Ottenere il frame dalla simulazione\n",
    "    frame = render_env.render()\n",
    "    if frame is not None:\n",
    "        cv2.imshow(\"HalfCheetah Simulation\", frame)  # Mostra il frame con OpenCV\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Premere 'q' per uscire dalla simulazione\n",
    "            break\n",
    "\n",
    "    time.sleep(0.01)  # Piccola pausa per migliorare la visualizzazione\n",
    "\n",
    "# Chiusura dell'ambiente e della finestra di visualizzazione\n",
    "render_env.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
