{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_HalfCheetah_tensorboard/PPO_17\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -362     |\n",
      "| time/              |          |\n",
      "|    fps             | 9027     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -308         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 7214         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092215715 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.5         |\n",
      "|    explained_variance   | -0.0073      |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 10.4         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 32.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-2.00 +/- 0.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -2         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00879291 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.5       |\n",
      "|    explained_variance   | 0.291      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 13.9       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 32.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -301     |\n",
      "| time/              |          |\n",
      "|    fps             | 5556     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -287         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5315         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074455566 |\n",
      "|    clip_fraction        | 0.0831       |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.52        |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 19.8         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00896     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-9.26 +/- 0.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -9.26       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008352937 |\n",
      "|    clip_fraction        | 0.0978      |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.52       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -296     |\n",
      "| time/              |          |\n",
      "|    fps             | 4852     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -290        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4832        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005432807 |\n",
      "|    clip_fraction        | 0.0243      |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.52       |\n",
      "|    explained_variance   | -0.157      |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 523         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00454    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.36e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -284         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4964         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075007505 |\n",
      "|    clip_fraction        | 0.0985       |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.53        |\n",
      "|    explained_variance   | 0.232        |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 28           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-30.44 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -30.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095256185 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.51        |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 8.8          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 20.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -272     |\n",
      "| time/              |          |\n",
      "|    fps             | 4623     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -263        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4648        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009063782 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.5        |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 23.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=3.54 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.54        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007911019 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.49       |\n",
      "|    explained_variance   | 0.663       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -257     |\n",
      "| time/              |          |\n",
      "|    fps             | 4565     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -250        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4689        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009140965 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.48       |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 24.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -239        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4748        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010312675 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.46       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 9.23        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-27.25 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -27.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01477962 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.43      |\n",
      "|    explained_variance   | 0.745      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 7.01       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    std                  | 0.985      |\n",
      "|    value_loss           | 15.6       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    fps             | 4594     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -205        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4629        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013508621 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.41       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 9.42        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 19.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-40.68 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -40.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0138631845 |\n",
      "|    clip_fraction        | 0.182        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.37        |\n",
      "|    explained_variance   | 0.74         |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 8.1          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 18.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    fps             | 4525     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -159      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4612      |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 28        |\n",
      "|    total_timesteps      | 131072    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0128852 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -8.36     |\n",
      "|    explained_variance   | 0.752     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 11.4      |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | -0.0275   |\n",
      "|    std                  | 0.973     |\n",
      "|    value_loss           | 23.1      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4656        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015620965 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.33       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 22.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-7.58 +/- 0.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -7.58      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 140000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02046784 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.3       |\n",
      "|    explained_variance   | 0.763      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 8.2        |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.965      |\n",
      "|    value_loss           | 17.6       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -113     |\n",
      "| time/              |          |\n",
      "|    fps             | 4586     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -84.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4653        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013597529 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.959       |\n",
      "|    value_loss           | 28.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-18.63 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -18.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014139539 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 28.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -58.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 4608     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -12.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4660        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016765399 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 28          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-7.98 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -7.98      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 180000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01967495 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.762      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 10.2       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    std                  | 0.946      |\n",
      "|    value_loss           | 23.5       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 21.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 4607     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 59.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4667        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015064225 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.941       |\n",
      "|    value_loss           | 36.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 91.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4715        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019031124 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-13.90 +/- 0.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -13.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020023908 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    std                  | 0.929       |\n",
      "|    value_loss           | 28.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 124      |\n",
      "| time/              |          |\n",
      "|    fps             | 4657     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 160         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018178556 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.924       |\n",
      "|    value_loss           | 32.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-27.19 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -27.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023521064 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 188      |\n",
      "| time/              |          |\n",
      "|    fps             | 4628     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 222        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4672       |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 229376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01996683 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.97      |\n",
      "|    explained_variance   | 0.714      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 16.9       |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0338    |\n",
      "|    std                  | 0.913      |\n",
      "|    value_loss           | 37.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 263         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021359755 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 40.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=367.87 +/- 19.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 368         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025562609 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.92       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    std                  | 0.903       |\n",
      "|    value_loss           | 32.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 305      |\n",
      "| time/              |          |\n",
      "|    fps             | 4657     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 341         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027122818 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.87       |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0386     |\n",
      "|    std                  | 0.897       |\n",
      "|    value_loss           | 29.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=662.65 +/- 32.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 663         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026284775 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.84       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    std                  | 0.893       |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 383      |\n",
      "| time/              |          |\n",
      "|    fps             | 4663     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 419         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4684        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024506863 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.81       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 17          |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    std                  | 0.89        |\n",
      "|    value_loss           | 39          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 451         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4727        |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026685031 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.78       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    std                  | 0.884       |\n",
      "|    value_loss           | 35.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=1172.07 +/- 21.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.17e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026618335 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.74       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 16          |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    std                  | 0.878       |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 480      |\n",
      "| time/              |          |\n",
      "|    fps             | 4692     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 516         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4704        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027815126 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.69       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    std                  | 0.871       |\n",
      "|    value_loss           | 35.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1488.27 +/- 29.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.49e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025827926 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.65       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.865       |\n",
      "|    value_loss           | 36.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 558      |\n",
      "| time/              |          |\n",
      "|    fps             | 4672     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 598         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4706        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025974017 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.62       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    std                  | 0.862       |\n",
      "|    value_loss           | 42          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 643         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4742        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 319488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027218826 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.58       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 18.3        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    std                  | 0.856       |\n",
      "|    value_loss           | 42.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=1826.57 +/- 42.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.83e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022601727 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.54       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 24          |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.85        |\n",
      "|    value_loss           | 52.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 688      |\n",
      "| time/              |          |\n",
      "|    fps             | 4719     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 735         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4752        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 335872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030658022 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.5        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 40.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=2076.84 +/- 181.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.08e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030145455 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.45       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    std                  | 0.838       |\n",
      "|    value_loss           | 49.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 813      |\n",
      "| time/              |          |\n",
      "|    fps             | 4716     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 865         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4749        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024786923 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.41       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 55.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=2387.33 +/- 48.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 2.39e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 360000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02410267 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.38      |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 23.2       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    std                  | 0.829      |\n",
      "|    value_loss           | 55.8       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 897      |\n",
      "| time/              |          |\n",
      "|    fps             | 4702     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 950         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4734        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024566464 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.34       |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.824       |\n",
      "|    value_loss           | 49.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 994         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4746        |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027705131 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.29       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    std                  | 0.817       |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=2382.89 +/- 38.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.38e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025200596 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.26       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 19.7        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    std                  | 0.813       |\n",
      "|    value_loss           | 46.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4721     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.08e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4738        |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020520564 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.23       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 24.7        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    std                  | 0.808       |\n",
      "|    value_loss           | 65.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=2333.78 +/- 75.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.33e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018624969 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.21       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 84.8        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00998    |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 235         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4698     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.15e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4724        |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012127158 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.21       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 146         |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 439         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.2e+03    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4742       |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 88         |\n",
      "|    total_timesteps      | 417792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03106182 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.18      |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 22.3       |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.802      |\n",
      "|    value_loss           | 54.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=2784.04 +/- 26.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.78e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 420000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030547522 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.796       |\n",
      "|    value_loss           | 55.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4722     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.29e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4746        |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023398738 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    std                  | 0.794       |\n",
      "|    value_loss           | 66.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=3011.12 +/- 24.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.01e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027492318 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.07       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 25.1        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0389     |\n",
      "|    std                  | 0.788       |\n",
      "|    value_loss           | 62.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4727     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.35e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4736        |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030227263 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.04       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.786       |\n",
      "|    value_loss           | 61.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.39e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4752       |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 96         |\n",
      "|    total_timesteps      | 458752     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02258322 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.02      |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.9       |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0403    |\n",
      "|    std                  | 0.783      |\n",
      "|    value_loss           | 66.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=3028.47 +/- 184.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 3.03e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 460000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02553604 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7         |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.9       |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0405    |\n",
      "|    std                  | 0.781      |\n",
      "|    value_loss           | 81.7       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4725     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.45e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4741        |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 475136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012979197 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.99       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 231         |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    std                  | 0.781       |\n",
      "|    value_loss           | 507         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=3180.88 +/- 103.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.18e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026242502 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.97       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 30.9        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.776       |\n",
      "|    value_loss           | 71.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4713     |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.57e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4721       |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 104        |\n",
      "|    total_timesteps      | 491520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03031031 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.94      |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.3       |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.774      |\n",
      "|    value_loss           | 80.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.61e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4738       |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 105        |\n",
      "|    total_timesteps      | 499712     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02983067 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.92      |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 33.4       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    std                  | 0.771      |\n",
      "|    value_loss           | 87.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=3270.22 +/- 71.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.27e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027111497 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.9        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 35.4        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.768       |\n",
      "|    value_loss           | 79.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4705     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.74e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4719        |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024632685 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.87       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.764       |\n",
      "|    value_loss           | 77.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=3429.28 +/- 118.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.43e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023504378 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.85       |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    std                  | 0.762       |\n",
      "|    value_loss           | 83.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4690     |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 1.8e+03   |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4708      |\n",
      "|    iterations           | 65        |\n",
      "|    time_elapsed         | 113       |\n",
      "|    total_timesteps      | 532480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0244323 |\n",
      "|    clip_fraction        | 0.24      |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -6.82     |\n",
      "|    explained_variance   | 0.952     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 37.3      |\n",
      "|    n_updates            | 640       |\n",
      "|    policy_gradient_loss | -0.0359   |\n",
      "|    std                  | 0.76      |\n",
      "|    value_loss           | 95.1      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=3464.45 +/- 66.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.46e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022398997 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.8        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 34          |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0384     |\n",
      "|    std                  | 0.757       |\n",
      "|    value_loss           | 87.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4679     |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.89e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4692        |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027348602 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.78       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    std                  | 0.755       |\n",
      "|    value_loss           | 74.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.94e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4706        |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028053906 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.75       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 31.1        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    std                  | 0.751       |\n",
      "|    value_loss           | 82.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=3632.60 +/- 16.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.63e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025647089 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.71       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 32.8        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    std                  | 0.744       |\n",
      "|    value_loss           | 89          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4686     |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 120      |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.06e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022923429 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.67       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 41.4        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.741       |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=3580.45 +/- 86.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.58e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022341192 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.65       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 37.8        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    std                  | 0.739       |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4664     |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.15e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4676        |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024953296 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.64       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 36          |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    std                  | 0.738       |\n",
      "|    value_loss           | 99.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.2e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4678        |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 598016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024716442 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.62       |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 39.8        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    std                  | 0.735       |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=3857.56 +/- 70.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 3.86e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 600000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01632211 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.6       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 53.4       |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0346    |\n",
      "|    std                  | 0.733      |\n",
      "|    value_loss           | 117        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4656     |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.26e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4672        |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 614400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023471389 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.58       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=3836.17 +/- 75.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.84e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 620000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024027724 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.57       |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 42.6        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 96.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4649     |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.35e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4667        |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 630784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023358444 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.56       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 29.8        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    std                  | 0.728       |\n",
      "|    value_loss           | 89.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.39e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4669        |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 136         |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030021716 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.53       |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.726       |\n",
      "|    value_loss           | 93.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=4014.73 +/- 102.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.01e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021705844 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 41.1        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4656     |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 647168   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.45e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4672        |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019063491 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 41.8        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    std                  | 0.72        |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=4312.77 +/- 77.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.31e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 660000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034482908 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.718       |\n",
      "|    value_loss           | 86          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4662     |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.54e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4677        |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 671744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024632948 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 45.6        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    std                  | 0.716       |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 2.57e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4691       |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 144        |\n",
      "|    total_timesteps      | 679936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02628667 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.43      |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 38         |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0365    |\n",
      "|    std                  | 0.715      |\n",
      "|    value_loss           | 95.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=4332.57 +/- 111.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.33e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 680000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022621242 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 45.3        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.712       |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4679     |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.66e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4687        |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023681726 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 37.9        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    std                  | 0.711       |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=4423.11 +/- 43.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.42e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 700000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025823377 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 46.5        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.708       |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4678     |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.74e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4693        |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 712704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031666018 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 36          |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    std                  | 0.706       |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=4531.96 +/- 50.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.53e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030409198 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.5        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 93.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4681     |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 2.83e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4695       |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 155        |\n",
      "|    total_timesteps      | 729088     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04509469 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.29      |\n",
      "|    explained_variance   | 0.722      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.3       |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    std                  | 0.699      |\n",
      "|    value_loss           | 88.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.87e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4704        |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028049134 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 56.5        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.698       |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=4737.21 +/- 33.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.74e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 740000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025323562 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.25       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 48.8        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    std                  | 0.695       |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.99e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4702        |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037523694 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.22       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.3        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    std                  | 0.693       |\n",
      "|    value_loss           | 98.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=4878.25 +/- 100.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.88e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 760000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021054596 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.19       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 66.3        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.688       |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4691     |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.08e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4697       |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 163        |\n",
      "|    total_timesteps      | 770048     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04075737 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.15      |\n",
      "|    explained_variance   | 0.526      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 30.1       |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0331    |\n",
      "|    std                  | 0.683      |\n",
      "|    value_loss           | 98.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.16e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4701        |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032924328 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.13       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 34.4        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.682       |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=5025.34 +/- 60.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.03e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038908906 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.1        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0346     |\n",
      "|    std                  | 0.679       |\n",
      "|    value_loss           | 93.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.24e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4699        |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 794624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019273046 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.07       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 58.9        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.677       |\n",
      "|    value_loss           | 146         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=5167.47 +/- 62.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.17e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047206793 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.06       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 23.4        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0366     |\n",
      "|    std                  | 0.676       |\n",
      "|    value_loss           | 94.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4689     |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 171      |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.33e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 811008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020390615 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.05       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.2        |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.34e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4702        |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025332933 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.04       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 53.2        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.674       |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=5468.82 +/- 87.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.47e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017946536 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.03       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 59          |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.672       |\n",
      "|    value_loss           | 143         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 827392   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.45e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4706        |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017947689 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 57.8        |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.671       |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=5463.76 +/- 22.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.46e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043719485 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6          |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.672       |\n",
      "|    value_loss           | 96.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4689     |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 179      |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.54e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4696        |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043676365 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 29          |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    std                  | 0.671       |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=5574.91 +/- 80.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.57e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031222098 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.98       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 43.4        |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.667       |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4688     |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.62e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4697       |\n",
      "|    iterations           | 106        |\n",
      "|    time_elapsed         | 184        |\n",
      "|    total_timesteps      | 868352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02920485 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.93      |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 40.9       |\n",
      "|    n_updates            | 1050       |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    std                  | 0.662      |\n",
      "|    value_loss           | 126        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.66e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4709        |\n",
      "|    iterations           | 107         |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 876544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024043687 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.91       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 63.6        |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.661       |\n",
      "|    value_loss           | 169         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=5661.08 +/- 44.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.66e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019123148 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.9        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 71          |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.66        |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4701     |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.72e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4705       |\n",
      "|    iterations           | 109        |\n",
      "|    time_elapsed         | 189        |\n",
      "|    total_timesteps      | 892928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02077879 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.89      |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 52.5       |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    std                  | 0.659      |\n",
      "|    value_loss           | 159        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=5831.48 +/- 67.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.83e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110804975 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.87       |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.657       |\n",
      "|    value_loss           | 62.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4691     |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.84e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4696       |\n",
      "|    iterations           | 111        |\n",
      "|    time_elapsed         | 193        |\n",
      "|    total_timesteps      | 909312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03566218 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.87      |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.3       |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.657      |\n",
      "|    value_loss           | 104        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.92e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4708        |\n",
      "|    iterations           | 112         |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044474684 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.84       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.3        |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.654       |\n",
      "|    value_loss           | 97.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=5829.18 +/- 121.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.83e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055641763 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.81       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 25.8        |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.652       |\n",
      "|    value_loss           | 81.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4697     |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 4e+03     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4702      |\n",
      "|    iterations           | 114       |\n",
      "|    time_elapsed         | 198       |\n",
      "|    total_timesteps      | 933888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0204705 |\n",
      "|    clip_fraction        | 0.314     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.8      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 53.4      |\n",
      "|    n_updates            | 1130      |\n",
      "|    policy_gradient_loss | -0.0183   |\n",
      "|    std                  | 0.652     |\n",
      "|    value_loss           | 175       |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=5800.62 +/- 122.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.8e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 940000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035768107 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.79       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 31.5        |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.651       |\n",
      "|    value_loss           | 95.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.08e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4688     |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 200      |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.1e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 202         |\n",
      "|    total_timesteps      | 950272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062293444 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.77       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.648       |\n",
      "|    value_loss           | 75.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.17e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4700        |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 958464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026896564 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.76       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 29.9        |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.647       |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=5750.13 +/- 411.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.75e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034481972 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.74       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 53.1        |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.645       |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 4.31e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4699      |\n",
      "|    iterations           | 119       |\n",
      "|    time_elapsed         | 207       |\n",
      "|    total_timesteps      | 974848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0352898 |\n",
      "|    clip_fraction        | 0.291     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.72     |\n",
      "|    explained_variance   | 0.717     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 33.5      |\n",
      "|    n_updates            | 1180      |\n",
      "|    policy_gradient_loss | -0.0293   |\n",
      "|    std                  | 0.643     |\n",
      "|    value_loss           | 114       |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=6207.42 +/- 71.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.21e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 980000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04967072 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.68      |\n",
      "|    explained_variance   | 0.697      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.6       |\n",
      "|    n_updates            | 1190       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.638      |\n",
      "|    value_loss           | 88.8       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4691     |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.46e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4701        |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 991232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041806318 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 32.4        |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.635       |\n",
      "|    value_loss           | 94.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4712        |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036548767 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.62       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 56.3        |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.633       |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=6348.27 +/- 70.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.35e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041141156 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.59       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    std                  | 0.628       |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.55e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4698     |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 1007616  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 4.61e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4703       |\n",
      "|    iterations           | 124        |\n",
      "|    time_elapsed         | 215        |\n",
      "|    total_timesteps      | 1015808    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07041606 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.53      |\n",
      "|    explained_variance   | 0.765      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 22.9       |\n",
      "|    n_updates            | 1230       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.624      |\n",
      "|    value_loss           | 79.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=6492.27 +/- 31.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.49e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017999135 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.00947    |\n",
      "|    std                  | 0.623       |\n",
      "|    value_loss           | 184         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4688     |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 1024000  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.7e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 1032192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014363175 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 247         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=6479.99 +/- 75.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.48e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013484834 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 117         |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    std                  | 0.621       |\n",
      "|    value_loss           | 270         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4682     |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 222      |\n",
      "|    total_timesteps | 1040384  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.77e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4692        |\n",
      "|    iterations           | 128         |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 1048576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023496803 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 52.2        |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 165         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.78e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4696        |\n",
      "|    iterations           | 129         |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 1056768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012880348 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.5        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 73.1        |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 227         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=6431.86 +/- 63.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | 6.43e+03  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1060000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0642121 |\n",
      "|    clip_fraction        | 0.37      |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.48     |\n",
      "|    explained_variance   | 0.996     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 18.1      |\n",
      "|    n_updates            | 1290      |\n",
      "|    policy_gradient_loss | -0.0356   |\n",
      "|    std                  | 0.619     |\n",
      "|    value_loss           | 77.4      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4686     |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 1064960  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 4.85e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4696       |\n",
      "|    iterations           | 131        |\n",
      "|    time_elapsed         | 228        |\n",
      "|    total_timesteps      | 1073152    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04488112 |\n",
      "|    clip_fraction        | 0.397      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.47      |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.7       |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    std                  | 0.618      |\n",
      "|    value_loss           | 90.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=6497.19 +/- 77.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.5e+03    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1080000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06079851 |\n",
      "|    clip_fraction        | 0.39       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.43      |\n",
      "|    explained_variance   | 0.743      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 24.9       |\n",
      "|    n_updates            | 1310       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.612      |\n",
      "|    value_loss           | 72.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4685     |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 1081344  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.9e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4693        |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 1089536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057512563 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.38       |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.609       |\n",
      "|    value_loss           | 78.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.96e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4692        |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 233         |\n",
      "|    total_timesteps      | 1097728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013581677 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.38       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 85.6        |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.00918    |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 239         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=6587.75 +/- 79.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 6.59e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1100000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0123555055 |\n",
      "|    clip_fraction        | 0.153        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -5.38        |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 73.9         |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0203      |\n",
      "|    std                  | 0.61         |\n",
      "|    value_loss           | 233          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4679     |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 1105920  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.01e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4683        |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 1114112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013977047 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.37       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 155         |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 286         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=6548.21 +/- 102.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.55e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025721103 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 38.9        |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 146         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.05e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4673     |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 1122304  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.06e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4681        |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 1130496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020474777 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 75.3        |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 196         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.07e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4679        |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 243         |\n",
      "|    total_timesteps      | 1138688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018614974 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 140         |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    std                  | 0.607       |\n",
      "|    value_loss           | 260         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=6631.86 +/- 133.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.63e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020846032 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    std                  | 0.607       |\n",
      "|    value_loss           | 187         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.1e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4671     |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 1146880  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.15e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4673       |\n",
      "|    iterations           | 141        |\n",
      "|    time_elapsed         | 247        |\n",
      "|    total_timesteps      | 1155072    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02217726 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.34      |\n",
      "|    explained_variance   | 0.86       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 56.6       |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.607      |\n",
      "|    value_loss           | 178        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=6669.92 +/- 122.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.67e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012782255 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 90.3        |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 339         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4667     |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 249      |\n",
      "|    total_timesteps | 1163264  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.15e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4675        |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 250         |\n",
      "|    total_timesteps      | 1171456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049334474 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 37.6        |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 200         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.17e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4674        |\n",
      "|    iterations           | 144         |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 1179648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010114375 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 109         |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.00912    |\n",
      "|    std                  | 0.609       |\n",
      "|    value_loss           | 301         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=6582.44 +/- 45.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.58e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1180000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048216008 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 79.8        |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 190         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.15e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4665     |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 1187840  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4668        |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 1196032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014888584 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 126         |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.609       |\n",
      "|    value_loss           | 244         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=6646.37 +/- 58.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.65e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011952142 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 73.8        |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 262         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4663     |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 1204224  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.12e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4667       |\n",
      "|    iterations           | 148        |\n",
      "|    time_elapsed         | 259        |\n",
      "|    total_timesteps      | 1212416    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03322653 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.35      |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 73.9       |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.61       |\n",
      "|    value_loss           | 149        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=6623.44 +/- 37.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | 6.62e+03  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1220000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0154896 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.35     |\n",
      "|    explained_variance   | 0.805     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 187       |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.0163   |\n",
      "|    std                  | 0.61      |\n",
      "|    value_loss           | 332       |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.13e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4651     |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 262      |\n",
      "|    total_timesteps | 1220608  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.16e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4657        |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 263         |\n",
      "|    total_timesteps      | 1228800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061620235 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 35.8        |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 90.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.21e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4657       |\n",
      "|    iterations           | 151        |\n",
      "|    time_elapsed         | 265        |\n",
      "|    total_timesteps      | 1236992    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08853138 |\n",
      "|    clip_fraction        | 0.479      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.34      |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 26.7       |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0236    |\n",
      "|    std                  | 0.609      |\n",
      "|    value_loss           | 71.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=5910.27 +/- 1378.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 5.91e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1240000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07478796 |\n",
      "|    clip_fraction        | 0.438      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.32      |\n",
      "|    explained_variance   | 0.813      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.3       |\n",
      "|    n_updates            | 1510       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    std                  | 0.606      |\n",
      "|    value_loss           | 72.4       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4651     |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 267      |\n",
      "|    total_timesteps | 1245184  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.21e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4653        |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 1253376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031689998 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.3        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 61.5        |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.604       |\n",
      "|    value_loss           | 209         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=6698.82 +/- 119.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.7e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024322102 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.29       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 61.3        |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    std                  | 0.603       |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4646     |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 1261568  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.21e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4654       |\n",
      "|    iterations           | 155        |\n",
      "|    time_elapsed         | 272        |\n",
      "|    total_timesteps      | 1269760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06707505 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.27      |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 26         |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.601      |\n",
      "|    value_loss           | 72.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.23e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4656        |\n",
      "|    iterations           | 156         |\n",
      "|    time_elapsed         | 274         |\n",
      "|    total_timesteps      | 1277952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030624658 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.25       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 57          |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.6         |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=6621.61 +/- 93.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.62e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062478513 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.22       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.597       |\n",
      "|    value_loss           | 66          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4650     |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 1286144  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.33e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4658       |\n",
      "|    iterations           | 158        |\n",
      "|    time_elapsed         | 277        |\n",
      "|    total_timesteps      | 1294336    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07641291 |\n",
      "|    clip_fraction        | 0.453      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.19      |\n",
      "|    explained_variance   | 0.852      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 23.5       |\n",
      "|    n_updates            | 1570       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.595      |\n",
      "|    value_loss           | 70.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=6161.74 +/- 1368.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.16e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031505346 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.18       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 75.6        |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.594       |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.36e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4649     |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 1302528  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.36e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4656        |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 1310720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018858576 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.16       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 104         |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.592       |\n",
      "|    value_loss           | 248         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.43e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 161         |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 1318912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019898362 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.15       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 54.7        |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    std                  | 0.59        |\n",
      "|    value_loss           | 220         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=6880.44 +/- 70.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.88e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1320000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09026857 |\n",
      "|    clip_fraction        | 0.43       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.11      |\n",
      "|    explained_variance   | 0.771      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 23.2       |\n",
      "|    n_updates            | 1610       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    std                  | 0.587      |\n",
      "|    value_loss           | 66.6       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.45e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4653     |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 285      |\n",
      "|    total_timesteps | 1327104  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.47e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4660       |\n",
      "|    iterations           | 163        |\n",
      "|    time_elapsed         | 286        |\n",
      "|    total_timesteps      | 1335296    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03930735 |\n",
      "|    clip_fraction        | 0.474      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.13      |\n",
      "|    explained_variance   | 0.741      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 62.1       |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.00293   |\n",
      "|    std                  | 0.591      |\n",
      "|    value_loss           | 123        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=6971.40 +/- 45.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.97e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1340000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06847571 |\n",
      "|    clip_fraction        | 0.423      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.11      |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 26.3       |\n",
      "|    n_updates            | 1630       |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    std                  | 0.587      |\n",
      "|    value_loss           | 62.4       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.5e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4653     |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 1343488  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.51e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4657       |\n",
      "|    iterations           | 165        |\n",
      "|    time_elapsed         | 290        |\n",
      "|    total_timesteps      | 1351680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03700964 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.08      |\n",
      "|    explained_variance   | 0.804      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 45.7       |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.027     |\n",
      "|    std                  | 0.584      |\n",
      "|    value_loss           | 102        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4664        |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 291         |\n",
      "|    total_timesteps      | 1359872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023589738 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.06       |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    std                  | 0.583       |\n",
      "|    value_loss           | 260         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=7020.69 +/- 157.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 7.02e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1360000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05724726 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.03      |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 31.8       |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.58       |\n",
      "|    value_loss           | 77.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.57e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4656     |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 293      |\n",
      "|    total_timesteps | 1368064  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.52e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 168         |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 1376256     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022797698 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.02       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 77.7        |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    std                  | 0.579       |\n",
      "|    value_loss           | 174         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=7096.19 +/- 121.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7.1e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019940924 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 40.1        |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.578       |\n",
      "|    value_loss           | 317         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.55e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4647     |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 297      |\n",
      "|    total_timesteps | 1384448  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.59e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4655       |\n",
      "|    iterations           | 170        |\n",
      "|    time_elapsed         | 299        |\n",
      "|    total_timesteps      | 1392640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02276326 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5         |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 128        |\n",
      "|    n_updates            | 1690       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.577      |\n",
      "|    value_loss           | 252        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=7000.83 +/- 42.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7e+03       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029939003 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.99       |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 82.4        |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.579       |\n",
      "|    value_loss           | 178         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.63e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4649     |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 301      |\n",
      "|    total_timesteps | 1400832  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.65e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4655       |\n",
      "|    iterations           | 172        |\n",
      "|    time_elapsed         | 302        |\n",
      "|    total_timesteps      | 1409024    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05296438 |\n",
      "|    clip_fraction        | 0.386      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.98      |\n",
      "|    explained_variance   | 0.81       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.6       |\n",
      "|    n_updates            | 1710       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.577      |\n",
      "|    value_loss           | 80.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.67e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4661        |\n",
      "|    iterations           | 173         |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 1417216     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022127194 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.96       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 49.6        |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.575       |\n",
      "|    value_loss           | 169         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=7078.77 +/- 61.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7.08e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1420000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040515065 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.95       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 47.1        |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    std                  | 0.576       |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.69e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4648     |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 306      |\n",
      "|    total_timesteps | 1425408  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.71e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4651       |\n",
      "|    iterations           | 175        |\n",
      "|    time_elapsed         | 308        |\n",
      "|    total_timesteps      | 1433600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05234484 |\n",
      "|    clip_fraction        | 0.421      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.93      |\n",
      "|    explained_variance   | 0.84       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 27.4       |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0252    |\n",
      "|    std                  | 0.573      |\n",
      "|    value_loss           | 69         |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=6904.91 +/- 109.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.9e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031007597 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 102         |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.571       |\n",
      "|    value_loss           | 205         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.7e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4646     |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 310      |\n",
      "|    total_timesteps | 1441792  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.77e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4652       |\n",
      "|    iterations           | 177        |\n",
      "|    time_elapsed         | 311        |\n",
      "|    total_timesteps      | 1449984    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05563052 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.89      |\n",
      "|    explained_variance   | 0.997      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 41.4       |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.57       |\n",
      "|    value_loss           | 80.1       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.79e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4655       |\n",
      "|    iterations           | 178        |\n",
      "|    time_elapsed         | 313        |\n",
      "|    total_timesteps      | 1458176    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05911491 |\n",
      "|    clip_fraction        | 0.433      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.85      |\n",
      "|    explained_variance   | 0.842      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 27         |\n",
      "|    n_updates            | 1770       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.566      |\n",
      "|    value_loss           | 67.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=7149.62 +/- 82.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7.15e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029348727 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.83       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 118         |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.565       |\n",
      "|    value_loss           | 160         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.81e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4652     |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 1466368  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.86e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 180         |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 1474560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060422387 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.8        |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 32.5        |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.563       |\n",
      "|    value_loss           | 66.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=7110.42 +/- 54.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 7.11e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1480000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04455567 |\n",
      "|    clip_fraction        | 0.467      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.78      |\n",
      "|    explained_variance   | 0.793      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 40.9       |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.00654   |\n",
      "|    std                  | 0.56       |\n",
      "|    value_loss           | 87.4       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.92e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4651     |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 318      |\n",
      "|    total_timesteps | 1482752  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.93e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 320         |\n",
      "|    total_timesteps      | 1490944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020402173 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.76       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 39.1        |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    std                  | 0.559       |\n",
      "|    value_loss           | 149         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.91e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4664        |\n",
      "|    iterations           | 183         |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 1499136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030475464 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.75       |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 52.6        |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.558       |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=7099.20 +/- 65.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 7.1e+03    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02354683 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.75      |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 74         |\n",
      "|    n_updates            | 1830       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.558      |\n",
      "|    value_loss           | 141        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.93e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4653     |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 323      |\n",
      "|    total_timesteps | 1507328  |\n",
      "---------------------------------\n",
      "Mean Reward: -260.46 ± 4.78\n"
     ]
    }
   ],
   "source": [
    "# Numero di ambienti paralleli per il training\n",
    "NUM_ENVS = 4\n",
    "\n",
    "\n",
    "# Wrapper personalizzato per modificare la ricompensa\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wrapper personalizzato che modifica la funzione di ricompensa dell'ambiente.\n",
    "\n",
    "    Penalizza il robot se il torso si inclina troppo all'indietro (indicando una caduta),\n",
    "    aumentando la penalità nel tempo finché rimane in questa posizione.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Esegue un passo nell'ambiente e modifica la ricompensa in base all'inclinazione del torso.\n",
    "\n",
    "        Args:\n",
    "            action (np.array): Azione scelta dall'agente.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Osservazione successiva, ricompensa modificata, stato di terminazione, stato di troncamento, info extra.\n",
    "        \"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        torso_angle = self.env.unwrapped.data.qpos[2]  # Angolo del torso\n",
    "\n",
    "        # Inizializza il timer della caduta se non esiste\n",
    "        if not hasattr(self, 'cappottato_start_time'):\n",
    "            self.cappottato_start_time = None\n",
    "\n",
    "        if torso_angle < -0.7:  # Se il robot è caduto\n",
    "            if self.cappottato_start_time is None:  # Se è la prima volta che cade\n",
    "                self.cappottato_start_time = time.time()  # Registra il tempo di inizio caduta\n",
    "\n",
    "            tempo_cappottato = time.time() - self.cappottato_start_time  # Tempo trascorso in stato di caduta\n",
    "            penalty = 50 * tempo_cappottato  # Penalità proporzionale al tempo caduto\n",
    "            reward -= penalty  # Sottrae la penalità alla ricompensa\n",
    "\n",
    "        else:  # Se il robot non è più in stato di caduta\n",
    "            self.cappottato_start_time = None  # Resetta il timer\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "# Funzione per creare un ambiente monitorato con la ricompensa personalizzata\n",
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce un'istanza dell'ambiente \"HalfCheetah-v5\" con parametri personalizzati,\n",
    "    monitoraggio delle prestazioni e applicazione del CustomRewardWrapper.\n",
    "\n",
    "    Returns:\n",
    "        function: Funzione che inizializza l'ambiente quando chiamata.\n",
    "    \"\"\"\n",
    "\n",
    "    def _init():\n",
    "        env = gym.make(\"HalfCheetah-v5\",\n",
    "                       reset_noise_scale=0.013459312664159742,  # Intensità del rumore iniziale\n",
    "                       forward_reward_weight=1.4435374113892951,  # Peso della ricompensa di avanzamento\n",
    "                       ctrl_cost_weight=0.09129087622076545)  # Peso del costo dell'azione\n",
    "        env = Monitor(env)  # Registra metriche dell'episodio\n",
    "        env = CustomRewardWrapper(env)  # Applica la ricompensa modificata\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "\n",
    "\n",
    "# Creazione degli ambienti per il training con parallelizzazione\n",
    "env = SubprocVecEnv([make_env() for _ in range(NUM_ENVS)])  # Parallelizza 4 ambienti\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)  # Normalizza solo le osservazioni\n",
    "\n",
    "# Selezione automatica della GPU se disponibile\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Parametri del modello PPO\n",
    "model_params = {\n",
    "    \"policy\": \"MlpPolicy\",  # Rete neurale per la politica\n",
    "    \"env\": env,  # Ambiente di training\n",
    "    \"learning_rate\": 0.0006365820963392328,  # Tasso di apprendimento ottimizzato\n",
    "    \"n_steps\": 2048,  # Passi prima di ogni aggiornamento\n",
    "    \"batch_size\": 1024,  # Dimensione del batch per l'update\n",
    "    \"n_epochs\": 10,  # Numero di epoche per aggiornare la politica\n",
    "    \"gamma\": 0.9932509667338772,  # Fattore di sconto per le ricompense future\n",
    "    \"gae_lambda\": 0.9196254842611007,  # Trade-off tra bias e varianza nel GAE\n",
    "    \"clip_range\": 0.19119739932498195,  # Clipping per la stabilità dell'aggiornamento\n",
    "    \"ent_coef\": 0.007152371678457134,  # Peso della perdita di entropia per incentivare l'esplorazione\n",
    "    \"verbose\": 1,  # Livello di log\n",
    "    \"tensorboard_log\": \"./ppo_HalfCheetah_tensorboard/\",  # Percorso di log per TensorBoard\n",
    "    \"device\": device,  # CPU o GPU\n",
    "    \"policy_kwargs\": dict(net_arch=[256, 256, 128])  # Architettura della rete neurale\n",
    "}\n",
    "\n",
    "# Creazione dell'ambiente di valutazione\n",
    "eval_env = DummyVecEnv([make_env()])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10.,\n",
    "                        training=False)  # Solo normalizzazione osservazioni\n",
    "\n",
    "# Callback per la valutazione e il salvataggio periodico del modello\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/best_model\",\n",
    "                             log_path=\"./logs/\", eval_freq=5000, deterministic=True, render=False)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=\"./logs/checkpoints/\",\n",
    "                                         name_prefix=\"ppo_halfcheetah_checkpoint\")\n",
    "\n",
    "# Creazione e addestramento del modello PPO\n",
    "model = PPO(**model_params)\n",
    "model.learn(total_timesteps=1_500_000, callback=CallbackList([eval_callback, checkpoint_callback]))\n",
    "\n",
    "# Salvataggio del modello e della normalizzazione dell'ambiente\n",
    "model.save(\"ppo_HalfCheetah_model\")\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")\n",
    "\n",
    "# Caricamento del modello e della normalizzazione per la valutazione\n",
    "model = PPO.load(\"ppo_HalfCheetah_model\", device=device)\n",
    "eval_env = VecNormalize.load(\"vecnormalize_HalfCheetah.pkl\", eval_env)\n",
    "eval_env.training = False  # Disattiva la normalizzazione durante la valutazione\n",
    "eval_env.reset()\n",
    "\n",
    "\n",
    "# Funzione per valutare l'agente su più episodi\n",
    "def evaluate_agent(model, env, episodes=100):\n",
    "    \"\"\"\n",
    "    Valuta le prestazioni dell'agente addestrato su un numero di episodi.\n",
    "\n",
    "    Args:\n",
    "        model (PPO): Modello PPO addestrato.\n",
    "        env (VecNormalize): Ambiente di valutazione.\n",
    "        episodes (int): Numero di episodi su cui eseguire la valutazione.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Media e deviazione standard della ricompensa ottenuta.\n",
    "    \"\"\"\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=episodes, deterministic=True)\n",
    "    print(f\"Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "# Valutazione del modello allenato\n",
    "mean_reward_trained, std_reward_trained = evaluate_agent(model, eval_env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Salviamo il modello e i parametri di normalizzazione\n",
    "\n",
    "# Salva il modello PPO addestrato per poterlo riutilizzare senza doverlo addestrare nuovamente.\n",
    "model.save(\"ppo_HalfCheetah_model\")\n",
    "\n",
    "# Salva lo stato dell'ambiente normalizzato, inclusi i parametri di normalizzazione delle osservazioni.\n",
    "# Questo è fondamentale per garantire che, quando il modello viene ricaricato, le osservazioni siano\n",
    "# preprocessate nello stesso modo usato durante l'addestramento.\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricando dati da: ./ppo_HalfCheetah_tensorboard/PPO_3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "# Percorso principale del log di TensorBoard\n",
    "log_dir = \"./ppo_HalfCheetah_tensorboard/\"\n",
    "\n",
    "# Selezione della sottocartella specifica contenente i dati del training\n",
    "# Cambiare il valore di `selected_subdir` per visualizzare i dati di altre sessioni di training.\n",
    "selected_subdir = \"PPO_3\"  # Opzioni possibili: \"PPO_1\", \"PPO_2\", ecc.\n",
    "\n",
    "# Costruzione del percorso completo della cartella selezionata\n",
    "selected_path = os.path.join(log_dir, selected_subdir)\n",
    "\n",
    "# Verifica che il percorso esista, altrimenti genera un errore\n",
    "if not os.path.exists(selected_path):\n",
    "    raise FileNotFoundError(f\"La cartella {selected_subdir} non esiste in {log_dir}\")\n",
    "\n",
    "print(f\"Caricando dati da: {selected_path}\")\n",
    "\n",
    "# Ricerca automatica del file degli eventi TensorBoard nella cartella selezionata\n",
    "event_file = None\n",
    "for root, dirs, files in os.walk(selected_path):\n",
    "    for file in files:\n",
    "        if \"events.out.tfevents\" in file:  # Identifica il file corretto\n",
    "            event_file = os.path.join(root, file)\n",
    "            break  # Esce dal loop dopo aver trovato il primo file valido\n",
    "\n",
    "# Se nessun file TensorBoard viene trovato, genera un errore\n",
    "if event_file is None:\n",
    "    raise FileNotFoundError(f\"Nessun file TensorBoard trovato in {selected_path}\")\n",
    "\n",
    "# Caricamento dei dati dal file TensorBoard\n",
    "event_acc = EventAccumulator(event_file)\n",
    "event_acc.Reload()  # Legge i dati memorizzati\n",
    "\n",
    "# Liste per memorizzare i timesteps e le ricompense medie durante il training\n",
    "timesteps = []\n",
    "mean_rewards = []\n",
    "\n",
    "# Estrazione dei dati dalla metrica \"rollout/ep_rew_mean\" (ricompensa media per episodio)\n",
    "for event in event_acc.Scalars(\"rollout/ep_rew_mean\"):\n",
    "    timesteps.append(event.step)  # Numero di timesteps corrispondente alla misurazione\n",
    "    mean_rewards.append(event.value)  # Ricompensa media registrata in quel momento\n",
    "\n",
    "# Creazione del grafico dell'andamento del training\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(timesteps, mean_rewards, label=\"Mean Reward\", color=\"blue\")  # Linea della ricompensa media\n",
    "plt.xlabel(\"Timesteps\")  # Etichetta asse X\n",
    "plt.ylabel(\"Mean Episodic Reward\")  # Etichetta asse Y\n",
    "plt.title(f\"Training Progress ({selected_subdir}): Reward vs Timesteps\")  # Titolo del grafico\n",
    "plt.legend()  # Mostra la legenda\n",
    "plt.grid(True)  # Attiva la griglia di sfondo per migliorare la leggibilità\n",
    "plt.show()  # Visualizza il grafico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Policy: Mean Reward: 7176.00 ± 92.17\n",
      "Random Policy: Mean Reward: -313.24 ± 120.58\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Percorsi dei file salvati del modello e della normalizzazione\n",
    "model_path = \"./ppo_HalfCheetah_model.zip\"\n",
    "vecnormalize_path = \"./vecnormalize_HalfCheetah.pkl\"\n",
    "\n",
    "# Controlla che i file esistano prima di caricarli\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Modello non trovato: {model_path}\")\n",
    "if not os.path.exists(vecnormalize_path):\n",
    "    raise FileNotFoundError(f\"File di normalizzazione non trovato: {vecnormalize_path}\")\n",
    "\n",
    "# Selezione automatica del device per l'esecuzione (GPU se disponibile, altrimenti CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Caricamento del modello PPO addestrato\n",
    "model = PPO.load(model_path, device=device)\n",
    "\n",
    "\n",
    "'''\n",
    "CustomRewardWrapper: Wrapper personalizzato per modificare la ricompensa\n",
    "- Penalizza il modello se il torso dell'agente si inclina oltre una certa soglia\n",
    "- La penalità aumenta in base al tempo trascorso in questa condizione\n",
    "'''\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Esegue un'azione nell'ambiente e applica una penalità se l'agente si inclina troppo.\n",
    "\n",
    "        - Se l'agente si inclina oltre una certa soglia (torso_angle < -0.7), viene applicata una penalità crescente\n",
    "          in base al tempo trascorso in questa condizione.\n",
    "        - La penalità viene aggiunta alla ricompensa originale per scoraggiare comportamenti indesiderati.\n",
    "        - Se l'agente si rialza, il timer della penalità viene azzerato.\n",
    "\n",
    "        Parametri:\n",
    "        - action: Azione da eseguire nell'ambiente.\n",
    "\n",
    "        Ritorna:\n",
    "        - obs: Nuovo stato dell'ambiente dopo l'azione.\n",
    "        - reward: Ricompensa modificata con eventuale penalità applicata.\n",
    "        - terminated: Indica se l'episodio è terminato.\n",
    "        - truncated: Indica se l'episodio è stato interrotto per altri motivi.\n",
    "        - info: Informazioni aggiuntive sull'episodio.\n",
    "        \"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Estrae l'angolo del torso dall'ambiente\n",
    "        torso_angle = self.env.unwrapped.data.qpos[2]\n",
    "\n",
    "        # Inizializza il timer al primo step\n",
    "        if not hasattr(self, 'cappottato_start_time'):\n",
    "            self.cappottato_start_time = None\n",
    "\n",
    "        # Se l'agente si è ribaltato (torso inclinato troppo)\n",
    "        if torso_angle < -0.7:\n",
    "            if self.cappottato_start_time is None:  # Se è la prima volta che cade\n",
    "                self.cappottato_start_time = time.time()  # Registra il tempo d'inizio\n",
    "\n",
    "            # Calcola il tempo totale passato in questa condizione\n",
    "            tempo_cappottato = time.time() - self.cappottato_start_time\n",
    "\n",
    "            # Applica una penalità crescente nel tempo\n",
    "            penalty = 50 * tempo_cappottato\n",
    "            reward -= penalty\n",
    "\n",
    "        else:  # Se l'agente si è ripreso, resetta il timer\n",
    "            self.cappottato_start_time = None\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "make_env: Funzione per creare un ambiente personalizzato\n",
    "- Imposta i parametri ottimizzati dell'ambiente\n",
    "- Applica un Monitor per tracciare le performance\n",
    "- Usa il wrapper CustomRewardWrapper per modificare la ricompensa\n",
    "'''\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(\"HalfCheetah-v5\",\n",
    "                        reset_noise_scale=0.013459312664159742,\n",
    "                        forward_reward_weight=1.4435374113892951,\n",
    "                        ctrl_cost_weight=0.09129087622076545)\n",
    "        env = Monitor(env)  # Registra le metriche dell'episodio\n",
    "        env = CustomRewardWrapper(env)  # Applica il wrapper personalizzato\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Creazione dell'ambiente di valutazione con normalizzazione caricata\n",
    "eval_env = DummyVecEnv([make_env()])\n",
    "eval_env = VecNormalize.load(vecnormalize_path, eval_env)  # Carica la normalizzazione salvata\n",
    "eval_env.training = False  # Disabilita la normalizzazione in fase di valutazione\n",
    "eval_env.reset()\n",
    "\n",
    "\n",
    "'''\n",
    "evaluate_random_policy: Funzione per valutare una policy casuale\n",
    "- Genera azioni casuali e le esegue nell'ambiente\n",
    "- Registra la ricompensa media ottenuta su più episodi\n",
    "'''\n",
    "def evaluate_random_policy(env, episodes=100):\n",
    "    total_rewards = []\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = [False] * env.num_envs  # Tiene traccia degli episodi terminati\n",
    "        episode_rewards = np.zeros(env.num_envs)  # Array per sommare le ricompense\n",
    "\n",
    "        while not all(done):  # Continua finché tutti gli episodi non sono finiti\n",
    "            actions = [env.action_space.sample() for _ in range(env.num_envs)]  # Azioni casuali\n",
    "            obs, rewards, done, _ = env.step(actions)\n",
    "            episode_rewards += rewards  # Accumula le ricompense\n",
    "\n",
    "        total_rewards.extend(episode_rewards)  # Memorizza le ricompense totali\n",
    "\n",
    "    # Calcola la media e la deviazione standard delle ricompense ottenute\n",
    "    mean_reward_random = np.mean(total_rewards)\n",
    "    std_reward_random = np.std(total_rewards)\n",
    "    return mean_reward_random, std_reward_random\n",
    "\n",
    "\n",
    "# Valutazione del modello addestrato\n",
    "mean_reward_trained, std_reward_trained = evaluate_policy(model, eval_env, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "# Valutazione di una policy casuale\n",
    "mean_reward_random, std_reward_random = evaluate_random_policy(eval_env, episodes=100)\n",
    "\n",
    "\n",
    "# Stampa dei risultati a confronto\n",
    "print(f\"Trained Policy: Mean Reward: {mean_reward_trained:.2f} ± {std_reward_trained:.2f}\")\n",
    "print(f\"Random Policy: Mean Reward: {mean_reward_random:.2f} ± {std_reward_random:.2f}\")\n",
    "\n",
    "\n",
    "# Creazione del grafico di confronto tra policy casuale e policy addestrata\n",
    "labels = ['Random Policy', 'Trained Policy']\n",
    "means = [mean_reward_random, mean_reward_trained]\n",
    "stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Mean Episodic Reward')  # Etichetta asse Y\n",
    "plt.title('Policy Comparison')  # Titolo del grafico\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
