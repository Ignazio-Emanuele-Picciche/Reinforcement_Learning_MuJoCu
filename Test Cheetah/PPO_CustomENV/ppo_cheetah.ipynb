{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_HalfCheetah_tensorboard/PPO_17\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -362     |\n",
      "| time/              |          |\n",
      "|    fps             | 9027     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -308         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 7214         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092215715 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.5         |\n",
      "|    explained_variance   | -0.0073      |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 10.4         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 32.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-2.00 +/- 0.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -2         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00879291 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.5       |\n",
      "|    explained_variance   | 0.291      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 13.9       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 32.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -301     |\n",
      "| time/              |          |\n",
      "|    fps             | 5556     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -287         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5315         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074455566 |\n",
      "|    clip_fraction        | 0.0831       |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.52        |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 19.8         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00896     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-9.26 +/- 0.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -9.26       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008352937 |\n",
      "|    clip_fraction        | 0.0978      |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.52       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -296     |\n",
      "| time/              |          |\n",
      "|    fps             | 4852     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -290        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4832        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005432807 |\n",
      "|    clip_fraction        | 0.0243      |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.52       |\n",
      "|    explained_variance   | -0.157      |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 523         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00454    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.36e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -284         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4964         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075007505 |\n",
      "|    clip_fraction        | 0.0985       |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.53        |\n",
      "|    explained_variance   | 0.232        |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 28           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-30.44 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -30.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095256185 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.51        |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 8.8          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 20.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -272     |\n",
      "| time/              |          |\n",
      "|    fps             | 4623     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -263        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4648        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009063782 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.5        |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 23.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=3.54 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.54        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007911019 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.49       |\n",
      "|    explained_variance   | 0.663       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -257     |\n",
      "| time/              |          |\n",
      "|    fps             | 4565     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -250        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4689        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009140965 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.48       |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 24.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -239        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4748        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010312675 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.46       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 9.23        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-27.25 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -27.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01477962 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.43      |\n",
      "|    explained_variance   | 0.745      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 7.01       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    std                  | 0.985      |\n",
      "|    value_loss           | 15.6       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    fps             | 4594     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -205        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4629        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013508621 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.41       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 9.42        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 19.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-40.68 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -40.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0138631845 |\n",
      "|    clip_fraction        | 0.182        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.37        |\n",
      "|    explained_variance   | 0.74         |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 8.1          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 18.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    fps             | 4525     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -159      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4612      |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 28        |\n",
      "|    total_timesteps      | 131072    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0128852 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -8.36     |\n",
      "|    explained_variance   | 0.752     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 11.4      |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | -0.0275   |\n",
      "|    std                  | 0.973     |\n",
      "|    value_loss           | 23.1      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4656        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015620965 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.33       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 22.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-7.58 +/- 0.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -7.58      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 140000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02046784 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.3       |\n",
      "|    explained_variance   | 0.763      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 8.2        |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.965      |\n",
      "|    value_loss           | 17.6       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -113     |\n",
      "| time/              |          |\n",
      "|    fps             | 4586     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -84.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4653        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013597529 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.959       |\n",
      "|    value_loss           | 28.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-18.63 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -18.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014139539 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 28.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -58.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 4608     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -12.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4660        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016765399 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 28          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-7.98 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -7.98      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 180000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01967495 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.762      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 10.2       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    std                  | 0.946      |\n",
      "|    value_loss           | 23.5       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 21.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 4607     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 59.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4667        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015064225 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.941       |\n",
      "|    value_loss           | 36.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 91.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4715        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019031124 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-13.90 +/- 0.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -13.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020023908 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    std                  | 0.929       |\n",
      "|    value_loss           | 28.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 124      |\n",
      "| time/              |          |\n",
      "|    fps             | 4657     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 160         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018178556 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.924       |\n",
      "|    value_loss           | 32.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-27.19 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -27.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023521064 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 188      |\n",
      "| time/              |          |\n",
      "|    fps             | 4628     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 222        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4672       |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 229376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01996683 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.97      |\n",
      "|    explained_variance   | 0.714      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 16.9       |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0338    |\n",
      "|    std                  | 0.913      |\n",
      "|    value_loss           | 37.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 263         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021359755 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 40.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=367.87 +/- 19.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 368         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025562609 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.92       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    std                  | 0.903       |\n",
      "|    value_loss           | 32.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 305      |\n",
      "| time/              |          |\n",
      "|    fps             | 4657     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 341         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027122818 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.87       |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0386     |\n",
      "|    std                  | 0.897       |\n",
      "|    value_loss           | 29.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=662.65 +/- 32.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 663         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026284775 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.84       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    std                  | 0.893       |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 383      |\n",
      "| time/              |          |\n",
      "|    fps             | 4663     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 419         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4684        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024506863 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.81       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 17          |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    std                  | 0.89        |\n",
      "|    value_loss           | 39          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 451         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4727        |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026685031 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.78       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    std                  | 0.884       |\n",
      "|    value_loss           | 35.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=1172.07 +/- 21.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.17e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026618335 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.74       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 16          |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    std                  | 0.878       |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 480      |\n",
      "| time/              |          |\n",
      "|    fps             | 4692     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 516         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4704        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027815126 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.69       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    std                  | 0.871       |\n",
      "|    value_loss           | 35.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1488.27 +/- 29.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.49e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025827926 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.65       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.865       |\n",
      "|    value_loss           | 36.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 558      |\n",
      "| time/              |          |\n",
      "|    fps             | 4672     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 598         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4706        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025974017 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.62       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    std                  | 0.862       |\n",
      "|    value_loss           | 42          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 643         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4742        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 319488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027218826 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.58       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 18.3        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    std                  | 0.856       |\n",
      "|    value_loss           | 42.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=1826.57 +/- 42.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.83e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022601727 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.54       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 24          |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.85        |\n",
      "|    value_loss           | 52.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 688      |\n",
      "| time/              |          |\n",
      "|    fps             | 4719     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 735         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4752        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 335872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030658022 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.5        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 40.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=2076.84 +/- 181.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.08e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030145455 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.45       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    std                  | 0.838       |\n",
      "|    value_loss           | 49.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 813      |\n",
      "| time/              |          |\n",
      "|    fps             | 4716     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 865         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4749        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024786923 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.41       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 55.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=2387.33 +/- 48.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 2.39e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 360000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02410267 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.38      |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 23.2       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    std                  | 0.829      |\n",
      "|    value_loss           | 55.8       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 897      |\n",
      "| time/              |          |\n",
      "|    fps             | 4702     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 950         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4734        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024566464 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.34       |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.824       |\n",
      "|    value_loss           | 49.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 994         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4746        |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027705131 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.29       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    std                  | 0.817       |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=2382.89 +/- 38.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.38e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025200596 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.26       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 19.7        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    std                  | 0.813       |\n",
      "|    value_loss           | 46.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4721     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.08e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4738        |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020520564 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.23       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 24.7        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    std                  | 0.808       |\n",
      "|    value_loss           | 65.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=2333.78 +/- 75.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.33e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018624969 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.21       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 84.8        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00998    |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 235         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4698     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.15e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4724        |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012127158 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.21       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 146         |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 439         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.2e+03    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4742       |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 88         |\n",
      "|    total_timesteps      | 417792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03106182 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.18      |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 22.3       |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.802      |\n",
      "|    value_loss           | 54.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=2784.04 +/- 26.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.78e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 420000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030547522 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.796       |\n",
      "|    value_loss           | 55.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4722     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.29e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4746        |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023398738 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    std                  | 0.794       |\n",
      "|    value_loss           | 66.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=3011.12 +/- 24.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.01e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027492318 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.07       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 25.1        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0389     |\n",
      "|    std                  | 0.788       |\n",
      "|    value_loss           | 62.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4727     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.35e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4736        |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030227263 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.04       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.786       |\n",
      "|    value_loss           | 61.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.39e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4752       |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 96         |\n",
      "|    total_timesteps      | 458752     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02258322 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.02      |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.9       |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0403    |\n",
      "|    std                  | 0.783      |\n",
      "|    value_loss           | 66.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=3028.47 +/- 184.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 3.03e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 460000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02553604 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7         |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.9       |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0405    |\n",
      "|    std                  | 0.781      |\n",
      "|    value_loss           | 81.7       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4725     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.45e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4741        |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 475136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012979197 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.99       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 231         |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    std                  | 0.781       |\n",
      "|    value_loss           | 507         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=3180.88 +/- 103.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.18e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026242502 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.97       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 30.9        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.776       |\n",
      "|    value_loss           | 71.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4713     |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.57e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4721       |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 104        |\n",
      "|    total_timesteps      | 491520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03031031 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.94      |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.3       |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.774      |\n",
      "|    value_loss           | 80.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.61e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4738       |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 105        |\n",
      "|    total_timesteps      | 499712     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02983067 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.92      |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 33.4       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    std                  | 0.771      |\n",
      "|    value_loss           | 87.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=3270.22 +/- 71.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.27e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027111497 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.9        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 35.4        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.768       |\n",
      "|    value_loss           | 79.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4705     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.74e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4719        |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024632685 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.87       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.764       |\n",
      "|    value_loss           | 77.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=3429.28 +/- 118.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.43e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023504378 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.85       |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    std                  | 0.762       |\n",
      "|    value_loss           | 83.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4690     |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 1.8e+03   |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4708      |\n",
      "|    iterations           | 65        |\n",
      "|    time_elapsed         | 113       |\n",
      "|    total_timesteps      | 532480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0244323 |\n",
      "|    clip_fraction        | 0.24      |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -6.82     |\n",
      "|    explained_variance   | 0.952     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 37.3      |\n",
      "|    n_updates            | 640       |\n",
      "|    policy_gradient_loss | -0.0359   |\n",
      "|    std                  | 0.76      |\n",
      "|    value_loss           | 95.1      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=3464.45 +/- 66.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.46e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022398997 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.8        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 34          |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0384     |\n",
      "|    std                  | 0.757       |\n",
      "|    value_loss           | 87.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4679     |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.89e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4692        |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027348602 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.78       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    std                  | 0.755       |\n",
      "|    value_loss           | 74.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.94e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4706        |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028053906 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.75       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 31.1        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    std                  | 0.751       |\n",
      "|    value_loss           | 82.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=3632.60 +/- 16.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.63e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025647089 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.71       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 32.8        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    std                  | 0.744       |\n",
      "|    value_loss           | 89          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4686     |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 120      |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.06e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022923429 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.67       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 41.4        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.741       |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=3580.45 +/- 86.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.58e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022341192 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.65       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 37.8        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    std                  | 0.739       |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4664     |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.15e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4676        |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024953296 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.64       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 36          |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    std                  | 0.738       |\n",
      "|    value_loss           | 99.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.2e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4678        |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 598016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024716442 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.62       |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 39.8        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    std                  | 0.735       |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=3857.56 +/- 70.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 3.86e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 600000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01632211 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.6       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 53.4       |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0346    |\n",
      "|    std                  | 0.733      |\n",
      "|    value_loss           | 117        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4656     |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.26e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4672        |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 614400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023471389 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.58       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=3836.17 +/- 75.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.84e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 620000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024027724 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.57       |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 42.6        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 96.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4649     |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.35e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4667        |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 630784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023358444 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.56       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 29.8        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    std                  | 0.728       |\n",
      "|    value_loss           | 89.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.39e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4669        |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 136         |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030021716 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.53       |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.726       |\n",
      "|    value_loss           | 93.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=4014.73 +/- 102.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.01e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021705844 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 41.1        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4656     |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 647168   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.45e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4672        |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019063491 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 41.8        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    std                  | 0.72        |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=4312.77 +/- 77.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.31e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 660000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034482908 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.718       |\n",
      "|    value_loss           | 86          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4662     |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.54e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4677        |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 671744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024632948 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 45.6        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    std                  | 0.716       |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 2.57e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4691       |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 144        |\n",
      "|    total_timesteps      | 679936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02628667 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.43      |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 38         |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0365    |\n",
      "|    std                  | 0.715      |\n",
      "|    value_loss           | 95.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=4332.57 +/- 111.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.33e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 680000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022621242 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 45.3        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.712       |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4679     |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.66e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4687        |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023681726 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 37.9        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    std                  | 0.711       |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=4423.11 +/- 43.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.42e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 700000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025823377 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 46.5        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.708       |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4678     |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.74e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4693        |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 712704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031666018 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 36          |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    std                  | 0.706       |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=4531.96 +/- 50.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.53e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030409198 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.5        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 93.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4681     |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 2.83e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4695       |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 155        |\n",
      "|    total_timesteps      | 729088     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04509469 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.29      |\n",
      "|    explained_variance   | 0.722      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.3       |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    std                  | 0.699      |\n",
      "|    value_loss           | 88.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.87e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4704        |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028049134 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 56.5        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.698       |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=4737.21 +/- 33.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.74e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 740000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025323562 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.25       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 48.8        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    std                  | 0.695       |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.99e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4702        |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037523694 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.22       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.3        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    std                  | 0.693       |\n",
      "|    value_loss           | 98.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=4878.25 +/- 100.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.88e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 760000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021054596 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.19       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 66.3        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.688       |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4691     |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.08e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4697       |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 163        |\n",
      "|    total_timesteps      | 770048     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04075737 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.15      |\n",
      "|    explained_variance   | 0.526      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 30.1       |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0331    |\n",
      "|    std                  | 0.683      |\n",
      "|    value_loss           | 98.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.16e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4701        |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032924328 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.13       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 34.4        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.682       |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=5025.34 +/- 60.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.03e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038908906 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.1        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0346     |\n",
      "|    std                  | 0.679       |\n",
      "|    value_loss           | 93.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.24e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4699        |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 794624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019273046 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.07       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 58.9        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.677       |\n",
      "|    value_loss           | 146         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=5167.47 +/- 62.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.17e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047206793 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.06       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 23.4        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0366     |\n",
      "|    std                  | 0.676       |\n",
      "|    value_loss           | 94.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4689     |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 171      |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.33e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 811008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020390615 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.05       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.2        |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.34e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4702        |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025332933 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.04       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 53.2        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.674       |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=5468.82 +/- 87.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.47e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017946536 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.03       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 59          |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.672       |\n",
      "|    value_loss           | 143         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 827392   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.45e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4706        |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017947689 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 57.8        |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.671       |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=5463.76 +/- 22.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.46e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043719485 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6          |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.672       |\n",
      "|    value_loss           | 96.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4689     |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 179      |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.54e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4696        |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043676365 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 29          |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    std                  | 0.671       |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=5574.91 +/- 80.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.57e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031222098 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.98       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 43.4        |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.667       |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4688     |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.62e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4697       |\n",
      "|    iterations           | 106        |\n",
      "|    time_elapsed         | 184        |\n",
      "|    total_timesteps      | 868352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02920485 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.93      |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 40.9       |\n",
      "|    n_updates            | 1050       |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    std                  | 0.662      |\n",
      "|    value_loss           | 126        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.66e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4709        |\n",
      "|    iterations           | 107         |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 876544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024043687 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.91       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 63.6        |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.661       |\n",
      "|    value_loss           | 169         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=5661.08 +/- 44.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.66e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019123148 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.9        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 71          |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.66        |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4701     |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.72e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4705       |\n",
      "|    iterations           | 109        |\n",
      "|    time_elapsed         | 189        |\n",
      "|    total_timesteps      | 892928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02077879 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.89      |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 52.5       |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    std                  | 0.659      |\n",
      "|    value_loss           | 159        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=5831.48 +/- 67.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.83e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110804975 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.87       |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.657       |\n",
      "|    value_loss           | 62.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4691     |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.84e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4696       |\n",
      "|    iterations           | 111        |\n",
      "|    time_elapsed         | 193        |\n",
      "|    total_timesteps      | 909312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03566218 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.87      |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.3       |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.657      |\n",
      "|    value_loss           | 104        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.92e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4708        |\n",
      "|    iterations           | 112         |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044474684 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.84       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.3        |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.654       |\n",
      "|    value_loss           | 97.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=5829.18 +/- 121.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.83e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055641763 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.81       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 25.8        |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.652       |\n",
      "|    value_loss           | 81.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4697     |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 4e+03     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4702      |\n",
      "|    iterations           | 114       |\n",
      "|    time_elapsed         | 198       |\n",
      "|    total_timesteps      | 933888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0204705 |\n",
      "|    clip_fraction        | 0.314     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.8      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 53.4      |\n",
      "|    n_updates            | 1130      |\n",
      "|    policy_gradient_loss | -0.0183   |\n",
      "|    std                  | 0.652     |\n",
      "|    value_loss           | 175       |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=5800.62 +/- 122.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.8e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 940000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035768107 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.79       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 31.5        |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.651       |\n",
      "|    value_loss           | 95.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.08e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4688     |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 200      |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.1e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 202         |\n",
      "|    total_timesteps      | 950272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062293444 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.77       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.648       |\n",
      "|    value_loss           | 75.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.17e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4700        |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 958464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026896564 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.76       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 29.9        |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.647       |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=5750.13 +/- 411.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.75e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034481972 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.74       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 53.1        |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.645       |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 4.31e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4699      |\n",
      "|    iterations           | 119       |\n",
      "|    time_elapsed         | 207       |\n",
      "|    total_timesteps      | 974848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0352898 |\n",
      "|    clip_fraction        | 0.291     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.72     |\n",
      "|    explained_variance   | 0.717     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 33.5      |\n",
      "|    n_updates            | 1180      |\n",
      "|    policy_gradient_loss | -0.0293   |\n",
      "|    std                  | 0.643     |\n",
      "|    value_loss           | 114       |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=6207.42 +/- 71.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.21e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 980000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04967072 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.68      |\n",
      "|    explained_variance   | 0.697      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.6       |\n",
      "|    n_updates            | 1190       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.638      |\n",
      "|    value_loss           | 88.8       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4691     |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.46e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4701        |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 991232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041806318 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 32.4        |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.635       |\n",
      "|    value_loss           | 94.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4712        |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036548767 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.62       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 56.3        |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.633       |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=6348.27 +/- 70.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.35e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041141156 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.59       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    std                  | 0.628       |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.55e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4698     |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 1007616  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 4.61e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4703       |\n",
      "|    iterations           | 124        |\n",
      "|    time_elapsed         | 215        |\n",
      "|    total_timesteps      | 1015808    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07041606 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.53      |\n",
      "|    explained_variance   | 0.765      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 22.9       |\n",
      "|    n_updates            | 1230       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.624      |\n",
      "|    value_loss           | 79.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=6492.27 +/- 31.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.49e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017999135 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.00947    |\n",
      "|    std                  | 0.623       |\n",
      "|    value_loss           | 184         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4688     |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 1024000  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.7e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 1032192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014363175 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 247         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=6479.99 +/- 75.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.48e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013484834 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 117         |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    std                  | 0.621       |\n",
      "|    value_loss           | 270         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4682     |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 222      |\n",
      "|    total_timesteps | 1040384  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.77e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4692        |\n",
      "|    iterations           | 128         |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 1048576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023496803 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 52.2        |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 165         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.78e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4696        |\n",
      "|    iterations           | 129         |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 1056768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012880348 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.5        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 73.1        |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 227         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=6431.86 +/- 63.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | 6.43e+03  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1060000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0642121 |\n",
      "|    clip_fraction        | 0.37      |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.48     |\n",
      "|    explained_variance   | 0.996     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 18.1      |\n",
      "|    n_updates            | 1290      |\n",
      "|    policy_gradient_loss | -0.0356   |\n",
      "|    std                  | 0.619     |\n",
      "|    value_loss           | 77.4      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4686     |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 1064960  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 4.85e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4696       |\n",
      "|    iterations           | 131        |\n",
      "|    time_elapsed         | 228        |\n",
      "|    total_timesteps      | 1073152    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04488112 |\n",
      "|    clip_fraction        | 0.397      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.47      |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.7       |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    std                  | 0.618      |\n",
      "|    value_loss           | 90.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=6497.19 +/- 77.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.5e+03    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1080000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06079851 |\n",
      "|    clip_fraction        | 0.39       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.43      |\n",
      "|    explained_variance   | 0.743      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 24.9       |\n",
      "|    n_updates            | 1310       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.612      |\n",
      "|    value_loss           | 72.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4685     |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 1081344  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.9e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4693        |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 1089536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057512563 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.38       |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.609       |\n",
      "|    value_loss           | 78.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.96e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4692        |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 233         |\n",
      "|    total_timesteps      | 1097728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013581677 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.38       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 85.6        |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.00918    |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 239         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=6587.75 +/- 79.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 6.59e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1100000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0123555055 |\n",
      "|    clip_fraction        | 0.153        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -5.38        |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 73.9         |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0203      |\n",
      "|    std                  | 0.61         |\n",
      "|    value_loss           | 233          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4679     |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 1105920  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.01e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4683        |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 1114112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013977047 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.37       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 155         |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 286         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=6548.21 +/- 102.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.55e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025721103 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 38.9        |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 146         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.05e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4673     |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 1122304  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.06e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4681        |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 1130496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020474777 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 75.3        |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 196         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.07e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4679        |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 243         |\n",
      "|    total_timesteps      | 1138688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018614974 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 140         |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    std                  | 0.607       |\n",
      "|    value_loss           | 260         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=6631.86 +/- 133.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.63e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020846032 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    std                  | 0.607       |\n",
      "|    value_loss           | 187         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.1e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4671     |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 1146880  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.15e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4673       |\n",
      "|    iterations           | 141        |\n",
      "|    time_elapsed         | 247        |\n",
      "|    total_timesteps      | 1155072    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02217726 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.34      |\n",
      "|    explained_variance   | 0.86       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 56.6       |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.607      |\n",
      "|    value_loss           | 178        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=6669.92 +/- 122.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.67e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012782255 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 90.3        |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 339         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4667     |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 249      |\n",
      "|    total_timesteps | 1163264  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.15e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4675        |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 250         |\n",
      "|    total_timesteps      | 1171456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049334474 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 37.6        |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 200         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.17e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4674        |\n",
      "|    iterations           | 144         |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 1179648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010114375 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 109         |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.00912    |\n",
      "|    std                  | 0.609       |\n",
      "|    value_loss           | 301         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=6582.44 +/- 45.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.58e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1180000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048216008 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 79.8        |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 190         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.15e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4665     |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 1187840  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4668        |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 1196032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014888584 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 126         |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.609       |\n",
      "|    value_loss           | 244         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=6646.37 +/- 58.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.65e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011952142 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 73.8        |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 262         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4663     |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 1204224  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.12e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4667       |\n",
      "|    iterations           | 148        |\n",
      "|    time_elapsed         | 259        |\n",
      "|    total_timesteps      | 1212416    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03322653 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.35      |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 73.9       |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.61       |\n",
      "|    value_loss           | 149        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=6623.44 +/- 37.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | 6.62e+03  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1220000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0154896 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.35     |\n",
      "|    explained_variance   | 0.805     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 187       |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.0163   |\n",
      "|    std                  | 0.61      |\n",
      "|    value_loss           | 332       |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.13e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4651     |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 262      |\n",
      "|    total_timesteps | 1220608  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.16e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4657        |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 263         |\n",
      "|    total_timesteps      | 1228800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061620235 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 35.8        |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 90.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.21e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4657       |\n",
      "|    iterations           | 151        |\n",
      "|    time_elapsed         | 265        |\n",
      "|    total_timesteps      | 1236992    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08853138 |\n",
      "|    clip_fraction        | 0.479      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.34      |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 26.7       |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0236    |\n",
      "|    std                  | 0.609      |\n",
      "|    value_loss           | 71.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=5910.27 +/- 1378.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 5.91e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1240000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07478796 |\n",
      "|    clip_fraction        | 0.438      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.32      |\n",
      "|    explained_variance   | 0.813      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.3       |\n",
      "|    n_updates            | 1510       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    std                  | 0.606      |\n",
      "|    value_loss           | 72.4       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4651     |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 267      |\n",
      "|    total_timesteps | 1245184  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.21e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4653        |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 1253376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031689998 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.3        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 61.5        |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.604       |\n",
      "|    value_loss           | 209         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=6698.82 +/- 119.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.7e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024322102 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.29       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 61.3        |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    std                  | 0.603       |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4646     |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 1261568  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.21e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4654       |\n",
      "|    iterations           | 155        |\n",
      "|    time_elapsed         | 272        |\n",
      "|    total_timesteps      | 1269760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06707505 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.27      |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 26         |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.601      |\n",
      "|    value_loss           | 72.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.23e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4656        |\n",
      "|    iterations           | 156         |\n",
      "|    time_elapsed         | 274         |\n",
      "|    total_timesteps      | 1277952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030624658 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.25       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 57          |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.6         |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=6621.61 +/- 93.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.62e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062478513 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.22       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.597       |\n",
      "|    value_loss           | 66          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4650     |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 1286144  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.33e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4658       |\n",
      "|    iterations           | 158        |\n",
      "|    time_elapsed         | 277        |\n",
      "|    total_timesteps      | 1294336    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07641291 |\n",
      "|    clip_fraction        | 0.453      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.19      |\n",
      "|    explained_variance   | 0.852      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 23.5       |\n",
      "|    n_updates            | 1570       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.595      |\n",
      "|    value_loss           | 70.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=6161.74 +/- 1368.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.16e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031505346 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.18       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 75.6        |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.594       |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.36e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4649     |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 1302528  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.36e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4656        |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 1310720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018858576 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.16       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 104         |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.592       |\n",
      "|    value_loss           | 248         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.43e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 161         |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 1318912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019898362 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.15       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 54.7        |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    std                  | 0.59        |\n",
      "|    value_loss           | 220         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=6880.44 +/- 70.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.88e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1320000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09026857 |\n",
      "|    clip_fraction        | 0.43       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.11      |\n",
      "|    explained_variance   | 0.771      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 23.2       |\n",
      "|    n_updates            | 1610       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    std                  | 0.587      |\n",
      "|    value_loss           | 66.6       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.45e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4653     |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 285      |\n",
      "|    total_timesteps | 1327104  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.47e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4660       |\n",
      "|    iterations           | 163        |\n",
      "|    time_elapsed         | 286        |\n",
      "|    total_timesteps      | 1335296    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03930735 |\n",
      "|    clip_fraction        | 0.474      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.13      |\n",
      "|    explained_variance   | 0.741      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 62.1       |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.00293   |\n",
      "|    std                  | 0.591      |\n",
      "|    value_loss           | 123        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=6971.40 +/- 45.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.97e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1340000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06847571 |\n",
      "|    clip_fraction        | 0.423      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.11      |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 26.3       |\n",
      "|    n_updates            | 1630       |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    std                  | 0.587      |\n",
      "|    value_loss           | 62.4       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.5e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4653     |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 1343488  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.51e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4657       |\n",
      "|    iterations           | 165        |\n",
      "|    time_elapsed         | 290        |\n",
      "|    total_timesteps      | 1351680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03700964 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.08      |\n",
      "|    explained_variance   | 0.804      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 45.7       |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.027     |\n",
      "|    std                  | 0.584      |\n",
      "|    value_loss           | 102        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4664        |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 291         |\n",
      "|    total_timesteps      | 1359872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023589738 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.06       |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    std                  | 0.583       |\n",
      "|    value_loss           | 260         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=7020.69 +/- 157.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 7.02e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1360000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05724726 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.03      |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 31.8       |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.58       |\n",
      "|    value_loss           | 77.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.57e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4656     |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 293      |\n",
      "|    total_timesteps | 1368064  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.52e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 168         |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 1376256     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022797698 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.02       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 77.7        |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    std                  | 0.579       |\n",
      "|    value_loss           | 174         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=7096.19 +/- 121.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7.1e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019940924 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 40.1        |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.578       |\n",
      "|    value_loss           | 317         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.55e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4647     |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 297      |\n",
      "|    total_timesteps | 1384448  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.59e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4655       |\n",
      "|    iterations           | 170        |\n",
      "|    time_elapsed         | 299        |\n",
      "|    total_timesteps      | 1392640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02276326 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5         |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 128        |\n",
      "|    n_updates            | 1690       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.577      |\n",
      "|    value_loss           | 252        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=7000.83 +/- 42.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7e+03       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029939003 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.99       |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 82.4        |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.579       |\n",
      "|    value_loss           | 178         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.63e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4649     |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 301      |\n",
      "|    total_timesteps | 1400832  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.65e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4655       |\n",
      "|    iterations           | 172        |\n",
      "|    time_elapsed         | 302        |\n",
      "|    total_timesteps      | 1409024    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05296438 |\n",
      "|    clip_fraction        | 0.386      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.98      |\n",
      "|    explained_variance   | 0.81       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.6       |\n",
      "|    n_updates            | 1710       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.577      |\n",
      "|    value_loss           | 80.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.67e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4661        |\n",
      "|    iterations           | 173         |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 1417216     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022127194 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.96       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 49.6        |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.575       |\n",
      "|    value_loss           | 169         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=7078.77 +/- 61.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7.08e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1420000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040515065 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.95       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 47.1        |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    std                  | 0.576       |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.69e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4648     |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 306      |\n",
      "|    total_timesteps | 1425408  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.71e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4651       |\n",
      "|    iterations           | 175        |\n",
      "|    time_elapsed         | 308        |\n",
      "|    total_timesteps      | 1433600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05234484 |\n",
      "|    clip_fraction        | 0.421      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.93      |\n",
      "|    explained_variance   | 0.84       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 27.4       |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0252    |\n",
      "|    std                  | 0.573      |\n",
      "|    value_loss           | 69         |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=6904.91 +/- 109.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.9e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031007597 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 102         |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.571       |\n",
      "|    value_loss           | 205         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.7e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4646     |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 310      |\n",
      "|    total_timesteps | 1441792  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.77e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4652       |\n",
      "|    iterations           | 177        |\n",
      "|    time_elapsed         | 311        |\n",
      "|    total_timesteps      | 1449984    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05563052 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.89      |\n",
      "|    explained_variance   | 0.997      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 41.4       |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.57       |\n",
      "|    value_loss           | 80.1       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.79e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4655       |\n",
      "|    iterations           | 178        |\n",
      "|    time_elapsed         | 313        |\n",
      "|    total_timesteps      | 1458176    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05911491 |\n",
      "|    clip_fraction        | 0.433      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.85      |\n",
      "|    explained_variance   | 0.842      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 27         |\n",
      "|    n_updates            | 1770       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.566      |\n",
      "|    value_loss           | 67.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=7149.62 +/- 82.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7.15e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029348727 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.83       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 118         |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.565       |\n",
      "|    value_loss           | 160         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.81e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4652     |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 1466368  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.86e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 180         |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 1474560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060422387 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.8        |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 32.5        |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.563       |\n",
      "|    value_loss           | 66.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=7110.42 +/- 54.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 7.11e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1480000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04455567 |\n",
      "|    clip_fraction        | 0.467      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.78      |\n",
      "|    explained_variance   | 0.793      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 40.9       |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.00654   |\n",
      "|    std                  | 0.56       |\n",
      "|    value_loss           | 87.4       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.92e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4651     |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 318      |\n",
      "|    total_timesteps | 1482752  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.93e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 320         |\n",
      "|    total_timesteps      | 1490944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020402173 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.76       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 39.1        |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    std                  | 0.559       |\n",
      "|    value_loss           | 149         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.91e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4664        |\n",
      "|    iterations           | 183         |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 1499136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030475464 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.75       |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 52.6        |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.558       |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=7099.20 +/- 65.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 7.1e+03    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02354683 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.75      |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 74         |\n",
      "|    n_updates            | 1830       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.558      |\n",
      "|    value_loss           | 141        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.93e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4653     |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 323      |\n",
      "|    total_timesteps | 1507328  |\n",
      "---------------------------------\n",
      "Mean Reward: -260.46  4.78\n"
     ]
    }
   ],
   "source": [
    "# Numero di ambienti paralleli per il training\n",
    "NUM_ENVS = 4\n",
    "\n",
    "# Wrapper personalizzato per applicare la ricompensa modificata\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        torso_angle = self.env.unwrapped.data.qpos[2]\n",
    "\n",
    "        if not hasattr(self, 'cappottato_start_time'):  # Inizializza al primo step\n",
    "            self.cappottato_start_time = None\n",
    "\n",
    "        if torso_angle < -0.7:  # Se  caduto\n",
    "            if self.cappottato_start_time is None:  # Se  la prima volta che cade\n",
    "                self.cappottato_start_time = time.time()  # Registra il tempo\n",
    "\n",
    "            tempo_cappottato = time.time() - self.cappottato_start_time # Calcola da quanto  cappottato\n",
    "\n",
    "            penalty = 50 * tempo_cappottato  # Penalit cumulativa (10  un fattore di scala)\n",
    "            reward -= penalty\n",
    "\n",
    "        else:  # Se non  pi caduto\n",
    "            self.cappottato_start_time = None  # Resetta il timer\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# Funzione per creare un ambiente monitorato con custom reward\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(\"HalfCheetah-v5\",\n",
    "                        reset_noise_scale=0.013459312664159742,\n",
    "                        forward_reward_weight=1.4435374113892951,\n",
    "                        ctrl_cost_weight=0.09129087622076545)\n",
    "        env = Monitor(env)\n",
    "        env = CustomRewardWrapper(env)  # Applica il custom reward\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Creazione degli ambienti per il training (con parallelizzazione)\n",
    "env = SubprocVecEnv([make_env() for _ in range(NUM_ENVS)])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)  # Normalizza solo osservazioni\n",
    "\n",
    "# Selezione automatica del device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Parametri del modello\n",
    "model_params = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env,\n",
    "    \"learning_rate\": 0.0006365820963392328,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 1024,\n",
    "    \"n_epochs\": 10,\n",
    "    \"gamma\": 0.9932509667338772,\n",
    "    \"gae_lambda\": 0.9196254842611007,\n",
    "    \"clip_range\": 0.19119739932498195,\n",
    "    \"ent_coef\": 0.007152371678457134,\n",
    "    \"verbose\": 1,\n",
    "    \"tensorboard_log\": \"./ppo_HalfCheetah_tensorboard/\",\n",
    "    \"device\": device,\n",
    "    \"policy_kwargs\": dict(net_arch=[256, 256, 128])\n",
    "}\n",
    "\n",
    "# Creazione dell'ambiente di valutazione con il custom reward\n",
    "eval_env = DummyVecEnv([make_env()])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10., training=False)\n",
    "\n",
    "# Callback per valutazione e salvataggi\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/best_model\",\n",
    "                             log_path=\"./logs/\", eval_freq=5000, deterministic=True, render=False)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=\"./logs/checkpoints/\",\n",
    "                                         name_prefix=\"ppo_halfcheetah_checkpoint\")\n",
    "\n",
    "# Creazione e training del modello\n",
    "model = PPO(**model_params)\n",
    "model.learn(total_timesteps=1_500_000, callback=CallbackList([eval_callback, checkpoint_callback]))\n",
    "\n",
    "# Salvataggio del modello e normalizzazione\n",
    "model.save(\"ppo_HalfCheetah_model\")\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")\n",
    "\n",
    "# Caricamento del modello e della normalizzazione per la valutazione\n",
    "model = PPO.load(\"ppo_HalfCheetah_model\", device=device)\n",
    "eval_env = VecNormalize.load(\"vecnormalize_HalfCheetah.pkl\", eval_env)\n",
    "eval_env.training = False\n",
    "eval_env.reset()\n",
    "\n",
    "# Funzione per la valutazione\n",
    "def evaluate_agent(model, env, episodes=100):\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=episodes, deterministic=True)\n",
    "    print(f\"Mean Reward: {mean_reward:.2f}  {std_reward:.2f}\")\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# Valutazione del modello allenato\n",
    "mean_reward_trained, std_reward_trained = evaluate_agent(model, eval_env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_HalfCheetah_model\")\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")    # salviamo anche i parametri di normalizzazione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricando dati da: ./ppo_HalfCheetah_tensorboard/PPO_17\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "# Percorso principale del log di TensorBoard\n",
    "log_dir = \"./ppo_HalfCheetah_tensorboard/\"\n",
    "\n",
    "# Specifica manualmente quale cartella usare (cambia il nome se vuoi un'altra versione)\n",
    "selected_subdir = \"PPO_3\"  # Cambia con \"PPO_1\" o \"PPO_2\" se necessario\n",
    "\n",
    "# Percorso completo della cartella selezionata\n",
    "selected_path = os.path.join(log_dir, selected_subdir)\n",
    "\n",
    "if not os.path.exists(selected_path):\n",
    "    raise FileNotFoundError(f\"La cartella {selected_subdir} non esiste in {log_dir}\")\n",
    "\n",
    "print(f\"Caricando dati da: {selected_path}\")\n",
    "\n",
    "# Trova il file degli eventi TensorBoard all'interno della cartella selezionata\n",
    "event_file = None\n",
    "for root, dirs, files in os.walk(selected_path):\n",
    "    for file in files:\n",
    "        if \"events.out.tfevents\" in file:\n",
    "            event_file = os.path.join(root, file)\n",
    "            break\n",
    "\n",
    "if event_file is None:\n",
    "    raise FileNotFoundError(f\"Nessun file TensorBoard trovato in {selected_path}\")\n",
    "\n",
    "# Carica i dati da TensorBoard\n",
    "event_acc = EventAccumulator(event_file)\n",
    "event_acc.Reload()\n",
    "\n",
    "# Estrai i timesteps e le rewards medie\n",
    "timesteps = []\n",
    "mean_rewards = []\n",
    "\n",
    "for event in event_acc.Scalars(\"rollout/ep_rew_mean\"):\n",
    "    timesteps.append(event.step)\n",
    "    mean_rewards.append(event.value)\n",
    "\n",
    "# Creazione del grafico\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(timesteps, mean_rewards, label=\"Mean Reward\", color=\"blue\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Mean Episodic Reward\")\n",
    "plt.title(f\"Training Progress ({selected_subdir}): Reward vs Timesteps\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Policy: Mean Reward: 7176.00  92.17\n",
      "Random Policy: Mean Reward: -313.24  120.58\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Percorsi dei file salvati\n",
    "model_path = \"./ppo_HalfCheetah_model.zip\"\n",
    "vecnormalize_path = \"./vecnormalize_HalfCheetah.pkl\"\n",
    "\n",
    "# Controlla se i file esistono\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Modello non trovato: {model_path}\")\n",
    "if not os.path.exists(vecnormalize_path):\n",
    "    raise FileNotFoundError(f\"File di normalizzazione non trovato: {vecnormalize_path}\")\n",
    "\n",
    "# Selezione automatica del device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Caricamento del modello addestrato\n",
    "model = PPO.load(model_path, device=device)\n",
    "\n",
    "# Wrapper personalizzato per applicare la ricompensa modificata\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        torso_angle = self.env.unwrapped.data.qpos[2]\n",
    "\n",
    "        if not hasattr(self, 'cappottato_start_time'):  # Inizializza al primo step\n",
    "            self.cappottato_start_time = None\n",
    "\n",
    "        if torso_angle < -0.7:  # Se  caduto\n",
    "            if self.cappottato_start_time is None:  # Se  la prima volta che cade\n",
    "                self.cappottato_start_time = time.time()  # Registra il tempo\n",
    "\n",
    "            tempo_cappottato = time.time() - self.cappottato_start_time # Calcola da quanto  cappottato\n",
    "\n",
    "            penalty = 50 * tempo_cappottato  # Penalit cumulativa (10  un fattore di scala)\n",
    "            reward -= penalty\n",
    "\n",
    "        else:  # Se non  pi caduto\n",
    "            self.cappottato_start_time = None  # Resetta il timer\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# Funzione per creare un ambiente monitorato con custom reward\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(\"HalfCheetah-v5\",\n",
    "                        reset_noise_scale=0.013459312664159742,\n",
    "                        forward_reward_weight=1.4435374113892951,\n",
    "                        ctrl_cost_weight=0.09129087622076545)\n",
    "        env = Monitor(env)\n",
    "        env = CustomRewardWrapper(env)  # Applica il custom reward\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "eval_env = DummyVecEnv([make_env()]) \n",
    "eval_env = VecNormalize.load(vecnormalize_path, eval_env)  # Carica la normalizzazione\n",
    "eval_env.training = False  # Disabilita la normalizzazione in fase di test\n",
    "eval_env.reset()\n",
    "\n",
    "# Funzione per valutare la policy casuale\n",
    "def evaluate_random_policy(env, episodes=100):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente vettorizzato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione (normalizzato e vettorizzato).\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = [False] * env.num_envs\n",
    "        episode_rewards = np.zeros(env.num_envs)\n",
    "        while not all(done):\n",
    "            actions = [env.action_space.sample() for _ in range(env.num_envs)]\n",
    "            obs, rewards, done, _ = env.step(actions)\n",
    "            episode_rewards += rewards\n",
    "        total_rewards.extend(episode_rewards)\n",
    "    \n",
    "    mean_reward_random = np.mean(total_rewards)\n",
    "    std_reward_random = np.std(total_rewards)\n",
    "    return mean_reward_random, std_reward_random\n",
    "\n",
    "# Valutazione della policy addestrata\n",
    "mean_reward_trained, std_reward_trained = evaluate_policy(model, eval_env, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "# Valutazione della policy casuale\n",
    "mean_reward_random, std_reward_random = evaluate_random_policy(eval_env, episodes=100)\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Trained Policy: Mean Reward: {mean_reward_trained:.2f}  {std_reward_trained:.2f}\")\n",
    "print(f\"Random Policy: Mean Reward: {mean_reward_random:.2f}  {std_reward_random:.2f}\")\n",
    "\n",
    "# Creazione del grafico di confronto\n",
    "labels = ['Random Policy', 'Trained Policy']\n",
    "means = [mean_reward_random, mean_reward_trained]\n",
    "stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Mean Episodic Reward')\n",
    "plt.title('Policy Comparison')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
