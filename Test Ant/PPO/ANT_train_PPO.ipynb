{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Training completo tra 5mln a 10mln di TimeStamp e tra 5000 e 10000 episodi\n",
    "\n",
    "-Per un tuning rapido da 500k a 1mln di TimeStamp e tra 500 a 1k episodi per trial (consigliati 500 trial)\n",
    "\n",
    "-Per i test preliminari 1mln di timestamp e 1k/2k episodi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHAT con search dice che per il train vanno bene anche 1mln di timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (PPO_4) -> {'reset_noise_scale': 0.16872520546404454, 'forward_reward_weight': 0.569165596187308, 'ctrl_cost_weight': 0.15369909636721105, 'healthy_reward': 1.1651483169773327, 'learning_rate': 0.00025118614395972893, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9900195327210904, 'gae_lambda': 0.8063306496367846, 'clip_range': 0.1411162146550987, 'ent_coef': 0.006226601057899701, 'variance_penalty_weight': 0.0007310600475679448}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (PPO_6) -> \n",
    "\n",
    "hp_reset_noise_scale=0.10405074414945424 # scala del rumore quando l'ambiente viene resettato \n",
    "\n",
    "hp_forward_reward_weight=0.5940601384640877 # peso del reward per il movimento in avanti\n",
    "\n",
    "hp_ctrl_cost_weight=0.14771040407991193 # peso del reward per il controllo\n",
    "\n",
    "hp_healthy_reward =1.4039427670916238 # reward per la salute\n",
    "\n",
    "\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "\n",
    "hp_learning_rate=0.00014010166026390974           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "\n",
    "hp_n_steps=4096                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "\n",
    "hp_batch_size=64                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "\n",
    "hp_gamma=0.9974446213345484      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "\n",
    "hp_gae_lambda=0.8025419607496327              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "\n",
    "hp_clip_range=0.16218657788555388               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "\n",
    "hp_ent_coef=0.00017603718662988996                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_7 {'reset_noise_scale': 0.16260110616284057, 'forward_reward_weight': 0.6594701821995568, 'ctrl_cost_weight': 0.13678469591501632, 'healthy_reward': 1.4384387807236847, 'contact_cost_weight': 0.0007721118603343064, 'healthy_z_lower': 0.11270460095319094, 'healthy_z_upper': 1.1367622027728483, 'contact_force_min': -0.8099290655891269, 'contact_force_max': 0.7683440461793597, 'learning_rate': 0.0001620494220647337, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9960403688730154, 'gae_lambda': 0.8519055821923104, 'clip_range': 0.28172421812629234, 'ent_coef': 0.015960745859518122, 'variance_penalty_weight': 0.011924537413547313}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_8 -> {'reset_noise_scale': 0.14953307712823055, 'forward_reward_weight': 0.5971580841907844, 'ctrl_cost_weight': 0.21085190913852067, 'healthy_reward': 1.3432502908397173, 'contact_cost_weight': 0.0006565424645557624, 'healthy_z_lower': 0.11576255546554826, 'healthy_z_upper': 1.0657755912005253, 'contact_force_min': -0.7947792512332761, 'contact_force_max': 0.7599774107257553, 'learning_rate': 0.0001417417141818677, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9977321276628237, 'gae_lambda': 0.8135998374897728, 'clip_range': 0.2502648636777115, 'ent_coef': 0.006686448422595028, 'variance_penalty_weight': 0.0008985044453683972}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_9 -> {'reset_noise_scale': 0.1224648700491494, 'forward_reward_weight': 1.0798217517026751, 'ctrl_cost_weight': 0.2788960190947023, 'healthy_reward': 1.4972086156641724, 'contact_cost_weight': 0.00019495257535118138, 'healthy_z_lower': 0.10525289571959973, 'healthy_z_upper': 1.1803240798353063, 'contact_force_min': -0.5187992701613672, 'contact_force_max': 0.5870857431066443, 'learning_rate': 0.000983439712869658, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9686135698396399, 'gae_lambda': 0.9145395422692033, 'clip_range': 0.37757085535729756, 'ent_coef': 0.00017055556769922042, 'std_penalty_weight': 0.28813167612676016}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_10 -> {'reset_noise_scale': 0.0811219889284557, 'forward_reward_weight': 0.794019967338759, 'ctrl_cost_weight': 0.1909084649203593, 'healthy_reward': 1.4695470159426132, 'contact_cost_weight': 0.00048075670076003045, 'healthy_z_lower': 0.19353492629665098, 'healthy_z_upper': 1.1936905952567158, 'contact_force_min': -0.5349939620294489, 'contact_force_max': 0.7307512698224117, 'learning_rate': 0.0003564760563058714, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9762294172462653, 'gae_lambda': 0.9261117656360015, 'clip_range': 0.3320669028429513, 'ent_coef': 0.0026780011357637598, 'std_penalty_weight': 0.20050838533111062}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_11 -> {'reset_noise_scale': 0.07145476067020312, 'forward_reward_weight': 1.545247241271003, 'ctrl_cost_weight': 0.5888754350371963, 'healthy_reward': 1.8961539637830271, 'contact_cost_weight': 0.000378389770529098, 'healthy_z_lower': 0.12761016035917702, 'healthy_z_upper': 1.0403641157003203, 'contact_force_min': -0.8542989596897922, 'contact_force_max': 0.948423208060974, 'learning_rate': 5.070148561650504e-05, 'n_steps': 6144, 'batch_size': 4096, 'gamma': 0.970695023374151, 'gae_lambda': 0.9257548804331505, 'clip_range': 0.17667299134062348, 'ent_coef': 0.15117794816431884}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_12 -> [I 2025-02-13 21:52:57,075] Trial 142 finished with value: 2147.46312115008 and parameters: {'reset_noise_scale': 0.05057132050677559, 'forward_reward_weight': 1.5242065847007638, 'ctrl_cost_weight': 1.0518615515373424, 'healthy_reward': 1.8906418906546967, 'contact_cost_weight': 0.00017255813970342036, 'healthy_z_lower': 0.171480519365306, 'healthy_z_upper': 0.9415199367799989, 'contact_force_min': -0.8951041717141913, 'contact_force_max': 0.9341249818927484, 'learning_rate': 0.0006673200204296133, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9635194700970308, 'gae_lambda': 0.9254828522668759, 'clip_range': 0.2273207023257754, 'ent_coef': 0.0003757232557716752}. Best is trial 142 with value: 2147.46312115008.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_13 -> [I 2025-02-13 23:00:25,932] Trial 220 finished with value: 2149.324657643946 and parameters: {'reset_noise_scale': 0.190758010402181, 'forward_reward_weight': 1.5237537263020415, 'ctrl_cost_weight': 0.8907565868390414, 'healthy_reward': 1.8601364542985845, 'contact_cost_weight': 0.0001613795517276871, 'healthy_z_lower': 0.15522435940425033, 'healthy_z_upper': 0.9593914101020591, 'contact_force_min': -0.9042700990109689, 'contact_force_max': 0.6524756726457444, 'learning_rate': 2.9911610221111873e-05, 'n_steps': 8192, 'batch_size': 512, 'gamma': 0.9623881975118631, 'gae_lambda': 0.9240000421659839, 'clip_range': 0.22260259988295936, 'ent_coef': 0.025157413751276267}. Best is trial 220 with value: 2149.324657643946.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PPO_14 -> [I 2025-02-14 13:03:50,697] Trial 84 finished with value: 2515.3490927833504 and parameters: {'reset_noise_scale': 0.05643887152139423, 'forward_reward_weight': 1.6571735389009492, 'ctrl_cost_weight': 1.2459307943421456, 'healthy_reward': 2.1887877456647273, 'contact_cost_weight': 3.359821608235945e-05, 'healthy_z_lower': 0.2931915606319033, 'healthy_z_upper': 1.1481704672437232, 'contact_force_min': -1.0300470905265697, 'contact_force_max': 1.0893204485382713, 'learning_rate': 4.741460073775156e-05, 'n_steps': 8192, 'batch_size': 512, 'gamma': 0.9544796539113963, 'gae_lambda': 0.9688396129147425, 'clip_range': 0.15375460930777995, 'ent_coef': 0.04630593810555868}. Best is trial 84 with value: 2515.3490927833504.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PPO_15 -> [I 2025-02-15 10:47:28,629] Trial 196 finished with value: 2681.5108748545686 and parameters: {'reset_noise_scale': 0.06976748570068636, 'forward_reward_weight': 1.7031840270142826, 'ctrl_cost_weight': 1.3736559851030032, 'healthy_reward': 2.370527088282008, 'contact_cost_weight': 5.099569789843523e-05, 'healthy_z_lower': 0.27002630801618377, 'healthy_z_upper': 1.2006270382609852, 'contact_force_min': -1.1373782536958372, 'contact_force_max': 0.9183153315908629, 'learning_rate': 0.00036529761855305995, 'n_steps': 4096, 'batch_size': 1024, 'gamma': 0.9373806202176476, 'gae_lambda': 0.9539161953969619, 'clip_range': 0.10327074757515992, 'ent_coef': 0.013922688117154487}. Best is trial 196 with value: 2681.5108748545686.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_16 -> [I 2025-02-15 15:11:08,399] Trial 119 finished with value: 2805.869046230279 and parameters: {'reset_noise_scale': 0.10340640550745006, 'forward_reward_weight': 1.7316798226755399, 'ctrl_cost_weight': 1.3431791463786378, 'healthy_reward': 2.4675748630080876, 'contact_cost_weight': 9.874201288446414e-05, 'healthy_z_lower': 0.3483299533071902, 'healthy_z_upper': 1.2738868349210701, 'contact_force_min': -1.1329009828313024, 'contact_force_max': 0.9421168936981189, 'learning_rate': 0.0008288719683865925, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.9591029149494671, 'gae_lambda': 0.9541845003001236, 'clip_range': 0.10331326443472709, 'ent_coef': 0.02661587350926783}. Best is trial 119 with value: 2805.869046230279.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_17 -> Sena iperparametri per il modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_18 -> con riduzione di batch size e learning rate\n",
    "\n",
    "[I 2025-02-21 11:38:03,685] Trial 270 finished with value: 2515.529357174921 and parameters: {'reset_noise_scale': 0.036384281716755174, 'forward_reward_weight': 1.6734584377802377, 'ctrl_cost_weight': 1.4114977503409765, 'healthy_reward': 2.3778485263135485, 'contact_cost_weight': 5.2035838720379083e-05, 'healthy_z_lower': 0.28072846666427437, 'healthy_z_upper': 1.0352095932018308, 'contact_force_min': -1.1644341511298375, 'contact_force_max': 1.0234607887152494, 'learning_rate': 3.1894181556364527e-06, 'n_steps': 10240, 'batch_size': 16, 'gamma': 0.9594058727562937, 'gae_lambda': 0.9620126823049164, 'clip_range': 0.14604633707952877, 'ent_coef': 0.06672931495998022}. Best is trial 270 with value: 2515.529357174921.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipreparametri dell'envrionment\n",
    "hp_reset_noise_scale= 0.036384281716755174 # scala del rumore quando l'ambiente viene resettato \n",
    "hp_forward_reward_weight = 1.6734584377802377 # peso del reward per il movimento in avanti\n",
    "hp_ctrl_cost_weight = 1.4114977503409765 # peso del reward per il controllo\n",
    "hp_healthy_reward = 2.3778485263135485 # reward per la salute\n",
    "\n",
    "hp_contact_cost_weight = 5.2035838720379083e-05\n",
    "healthy_z = (0.28072846666427437, 1.0352095932018308)\n",
    "contact_force = (-1.1644341511298375, 1.0234607887152494)\n",
    "\n",
    "\n",
    "#   normalize: true\n",
    "#   n_envs: 1\n",
    "#   policy: 'MlpPolicy'\n",
    "#   n_timesteps: !!float 1e7\n",
    "#   batch_size: 32\n",
    "#   n_steps: 512\n",
    "#   gamma: 0.98\n",
    "#   learning_rate: 1.90609e-05\n",
    "#   ent_coef: 4.9646e-07\n",
    "#   clip_range: 0.1\n",
    "#   n_epochs: 10\n",
    "#   gae_lambda: 0.8\n",
    "#   max_grad_norm: 0.6\n",
    "#   vf_coef: 0.677239\n",
    "\n",
    "\n",
    "# Iperparametri del modello/policy\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "hp_learning_rate=3.1894181556364527e-06          # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "hp_n_steps=10240                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "hp_batch_size=16                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "hp_gamma=0.9594058727562937      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "hp_gae_lambda=0.9620126823049164              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "hp_clip_range=0.14604633707952877               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "hp_ent_coef=0.06672931495998022                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrà effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=hp_reset_noise_scale, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=hp_forward_reward_weight, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=hp_ctrl_cost_weight, # peso del reward per il controllo\n",
    "                    healthy_reward =hp_healthy_reward, # reward per la salute\n",
    "                    contact_cost_weight=hp_contact_cost_weight,\n",
    "                    healthy_z_range=healthy_z,\n",
    "                    contact_force_range=contact_force,\n",
    "                    render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire più istanze dell'ambiente come se fossero una singola entità.\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "#env = DummyVecEnv([make_env])  \n",
    "\n",
    "\n",
    "# Inizializza un ambiente vettoriale con 8 istanze dell'ambiente Ant-v5.\n",
    "# Questo è utile per parallelizzare l'ambiente per accelerare l'allenamento.\n",
    "NUM_ENVS=8\n",
    "env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def linear_schedule(initial_value):\n",
    "#     def func(progress_remaining):\n",
    "#         return progress_remaining * initial_value\n",
    "#     return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Definiamo il modello RL (PPO) con spiegazioni dettagliate per ciascun parametro\n",
    "\n",
    "model = PPO(\n",
    "    policy=hp_policy,           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "    env=env,                     # Ambiente di addestramento: usa l'ambiente vettorializzato e normalizzato creato in precedenza\n",
    "    # learning_rate=hp_learning_rate,  # usa lo scheduler lineare\n",
    "    # n_steps=hp_n_steps,                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "    # batch_size=hp_batch_size,                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "    n_epochs=hp_n_epochs,                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "    # gamma=hp_gamma,      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "    # gae_lambda=hp_gae_lambda,              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "    # clip_range=hp_clip_range,               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "    # ent_coef=hp_ent_coef,                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n",
    "    # #seed=42,                        # Seed per la riproducibilità\n",
    "    verbose=0,                    # Livello di verbosità: 1 per stampare informazioni di log utili durante l'addestramento\n",
    "    tensorboard_log=\"./ppo_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "    device='mps'                    # Specifica l'uso della GPU su Apple Silicon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Creiamo un ambiente vettorializzato per la valutazione\n",
    "# Utilizziamo SubprocVecEnv per gestire più istanze dell'ambiente in parallelo.\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "eval_env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "# Normalizziamo osservazioni (obs) e ricompense (reward) per l'ambiente di valutazione\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback per la valutazione del modello durante l'allenamento\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,  # Ambiente di valutazione\n",
    "    best_model_save_path=\"./logs/best_model\",  # Percorso per salvare il miglior modello\n",
    "    log_path=\"./logs/\",  # Percorso per salvare i log\n",
    "    eval_freq=25000,  # Frequenza di valutazione (ogni 25000 timesteps)\n",
    "    deterministic=True,  # Esegui azioni deterministiche durante la valutazione\n",
    "    render=False  # Non mostrare il rendering durante la valutazione\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=200000, episode_reward=2.17 +/- 1.41\n",
      "Episode length: 65.00 +/- 45.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=0.12 +/- 0.13\n",
      "Episode length: 13.60 +/- 2.94\n",
      "Eval num_timesteps=600000, episode_reward=0.02 +/- 0.10\n",
      "Episode length: 11.40 +/- 1.36\n",
      "Eval num_timesteps=800000, episode_reward=0.39 +/- 0.50\n",
      "Episode length: 17.20 +/- 7.78\n",
      "Eval num_timesteps=1000000, episode_reward=1.23 +/- 0.55\n",
      "Episode length: 23.20 +/- 5.95\n",
      "Eval num_timesteps=1200000, episode_reward=2.31 +/- 0.78\n",
      "Episode length: 42.00 +/- 28.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1400000, episode_reward=1.34 +/- 0.25\n",
      "Episode length: 22.20 +/- 5.84\n",
      "Eval num_timesteps=1600000, episode_reward=3.07 +/- 1.19\n",
      "Episode length: 49.80 +/- 20.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1800000, episode_reward=3.00 +/- 0.57\n",
      "Episode length: 48.60 +/- 7.68\n",
      "Eval num_timesteps=2000000, episode_reward=4.06 +/- 1.21\n",
      "Episode length: 75.40 +/- 18.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2200000, episode_reward=2.64 +/- 0.75\n",
      "Episode length: 48.00 +/- 18.46\n",
      "Eval num_timesteps=2400000, episode_reward=4.77 +/- 1.41\n",
      "Episode length: 95.60 +/- 41.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2600000, episode_reward=7.11 +/- 2.45\n",
      "Episode length: 142.00 +/- 68.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2800000, episode_reward=15.48 +/- 10.99\n",
      "Episode length: 344.00 +/- 242.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000000, episode_reward=25.39 +/- 15.70\n",
      "Episode length: 615.40 +/- 376.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3200000, episode_reward=28.98 +/- 13.91\n",
      "Episode length: 745.60 +/- 364.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3400000, episode_reward=24.98 +/- 13.82\n",
      "Episode length: 695.20 +/- 375.56\n",
      "Eval num_timesteps=3600000, episode_reward=24.08 +/- 17.99\n",
      "Episode length: 621.80 +/- 464.09\n",
      "Eval num_timesteps=3800000, episode_reward=25.30 +/- 7.67\n",
      "Episode length: 687.80 +/- 207.08\n",
      "Eval num_timesteps=4000000, episode_reward=32.62 +/- 6.82\n",
      "Episode length: 864.20 +/- 166.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4200000, episode_reward=16.74 +/- 15.71\n",
      "Episode length: 466.40 +/- 442.37\n",
      "Eval num_timesteps=4400000, episode_reward=25.46 +/- 11.65\n",
      "Episode length: 740.80 +/- 336.61\n",
      "Eval num_timesteps=4600000, episode_reward=21.14 +/- 14.27\n",
      "Episode length: 600.00 +/- 387.58\n",
      "Eval num_timesteps=4800000, episode_reward=17.54 +/- 12.82\n",
      "Episode length: 519.60 +/- 367.51\n",
      "Eval num_timesteps=5000000, episode_reward=26.47 +/- 9.62\n",
      "Episode length: 778.40 +/- 271.64\n",
      "Eval num_timesteps=5200000, episode_reward=19.17 +/- 11.65\n",
      "Episode length: 589.40 +/- 367.18\n",
      "Eval num_timesteps=5400000, episode_reward=33.45 +/- 1.45\n",
      "Episode length: 988.40 +/- 23.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5600000, episode_reward=15.58 +/- 14.79\n",
      "Episode length: 472.80 +/- 432.57\n",
      "Eval num_timesteps=5800000, episode_reward=23.46 +/- 11.34\n",
      "Episode length: 725.40 +/- 336.38\n",
      "Eval num_timesteps=6000000, episode_reward=28.62 +/- 9.35\n",
      "Episode length: 866.40 +/- 267.20\n",
      "Eval num_timesteps=6200000, episode_reward=15.04 +/- 9.30\n",
      "Episode length: 466.00 +/- 289.90\n",
      "Eval num_timesteps=6400000, episode_reward=23.51 +/- 8.34\n",
      "Episode length: 739.20 +/- 245.17\n",
      "Eval num_timesteps=6600000, episode_reward=20.19 +/- 15.18\n",
      "Episode length: 627.20 +/- 456.98\n",
      "Eval num_timesteps=6800000, episode_reward=19.48 +/- 9.90\n",
      "Episode length: 633.80 +/- 317.45\n",
      "Eval num_timesteps=7000000, episode_reward=30.51 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=7200000, episode_reward=18.04 +/- 12.48\n",
      "Episode length: 590.40 +/- 396.04\n",
      "Eval num_timesteps=7400000, episode_reward=23.87 +/- 9.83\n",
      "Episode length: 793.00 +/- 316.85\n",
      "Eval num_timesteps=7600000, episode_reward=30.58 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=7800000, episode_reward=30.58 +/- 0.96\n",
      "Episode length: 999.40 +/- 1.20\n",
      "Eval num_timesteps=8000000, episode_reward=17.44 +/- 10.87\n",
      "Episode length: 601.00 +/- 367.22\n",
      "Eval num_timesteps=8200000, episode_reward=26.90 +/- 5.61\n",
      "Episode length: 912.00 +/- 176.00\n",
      "Eval num_timesteps=8400000, episode_reward=19.21 +/- 11.72\n",
      "Episode length: 624.00 +/- 372.22\n",
      "Eval num_timesteps=8600000, episode_reward=18.79 +/- 14.39\n",
      "Episode length: 621.60 +/- 463.78\n",
      "Eval num_timesteps=8800000, episode_reward=18.21 +/- 8.64\n",
      "Episode length: 634.80 +/- 315.54\n",
      "Eval num_timesteps=9000000, episode_reward=17.31 +/- 11.80\n",
      "Episode length: 568.40 +/- 383.70\n",
      "Eval num_timesteps=9200000, episode_reward=29.51 +/- 1.59\n",
      "Episode length: 978.80 +/- 42.40\n",
      "Eval num_timesteps=9400000, episode_reward=29.80 +/- 1.31\n",
      "Episode length: 978.40 +/- 43.20\n",
      "Eval num_timesteps=9600000, episode_reward=29.80 +/- 1.57\n",
      "Episode length: 972.40 +/- 35.67\n",
      "Eval num_timesteps=9800000, episode_reward=20.09 +/- 10.17\n",
      "Episode length: 652.60 +/- 325.92\n",
      "Eval num_timesteps=10000000, episode_reward=30.06 +/- 0.38\n",
      "Episode length: 1000.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x16ad47a00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = 10000000  # Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\n",
    "model.learn(total_timesteps=total_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model_PPO18\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200-400 episodi sono adeguati \n",
    "# def evaluate_policy(env, policy, episodes=500):\n",
    "#     \"\"\"\n",
    "#     Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "#     Parametri:\n",
    "#     - env: L'ambiente di simulazione.\n",
    "#     - policy: La policy addestrata da valutare.\n",
    "#     - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "#     Ritorna:\n",
    "#     - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "#     \"\"\"\n",
    "#     total_rewards = []\n",
    "#     for _ in range(episodes):\n",
    "#         obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "#         while not done:\n",
    "#             action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "#             obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "#             total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "#         total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "#     return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_random_policy(env, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = [False] * env.num_envs  # Stato di completamento per ogni istanza dell'ambiente\n",
    "        episode_rewards = np.zeros(env.num_envs)  # Ricompense accumulate per ogni istanza dell'ambiente\n",
    "        while not all(done):\n",
    "            actions = [env.action_space.sample() for _ in range(env.num_envs)]  # Azioni casuali per ogni istanza dell'ambiente\n",
    "            obs, rewards, done, infos = env.step(actions)  # Esegue le azioni e ottiene il feedback dall'ambiente\n",
    "            episode_rewards += rewards  # Accumula le ricompense ottenute\n",
    "        total_rewards.extend(episode_rewards)  # Aggiunge le ricompense totali dell'episodio alla lista\n",
    "    mean_reward_random = np.mean(total_rewards)  # Calcola la ricompensa media\n",
    "    # std_reward_random = np.std(total_rewards)  # Calcola la deviazione standard delle ricompense (commentato)\n",
    "    return mean_reward_random  # Ritorna la ricompensa media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.training = False # Setta l'environment in modalità di valutazione\n",
    "# env.norm_reward = False # Disabilita la normalizzazione della reward. Questo è importante per valutare correttamente il modello.\n",
    "\n",
    "# # Valutazione dopo l'addestramento\n",
    "# mean_reward_trained, std_reward_trained = evaluate_policy(model, env, n_eval_episodes=500)  # Valuta la policy addestrata\n",
    "# mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# # Stampa dei risultati\n",
    "# print(f\"Trained Policy: Mean Reward: {mean_reward_trained}\")\n",
    "# print(f\"Random Policy: Mean Reward: {mean_reward_random}\")\n",
    "\n",
    "# # Creazione del grafico di confronto\n",
    "# # labels = ['Random Policy', 'Trained Policy']\n",
    "# # means = [mean_reward_random, mean_reward_trained]\n",
    "# # stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "# # plt.figure(figsize=(8, 5))\n",
    "# # plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "# # plt.ylabel('Mean Episodic Reward')\n",
    "# # plt.title('Policy Comparison')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
