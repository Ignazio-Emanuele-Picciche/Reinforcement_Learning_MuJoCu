{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri dell'ambiente\n",
    "hp_reset_noise_scale = 0.036384281716755174  # Scala del rumore quando l'ambiente viene resettato\n",
    "hp_forward_reward_weight = 1.6734584377802377  # Peso del reward per il movimento in avanti\n",
    "hp_ctrl_cost_weight = 1.4114977503409765  # Peso del reward per il controllo\n",
    "hp_healthy_reward = 2.3778485263135485  # Reward per la salute\n",
    "\n",
    "hp_contact_cost_weight = 5.2035838720379083e-05  # Peso del costo di contatto\n",
    "healthy_z = (0.28072846666427437, 1.0352095932018308)  # Intervallo di altezza considerato sano\n",
    "contact_force = (-1.1644341511298375, 1.0234607887152494)  # Intervallo di forza di contatto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_render = gym.make(\n",
    "    \"Ant-v5\",\n",
    "    reset_noise_scale=hp_reset_noise_scale,\n",
    "    forward_reward_weight=hp_forward_reward_weight,\n",
    "    ctrl_cost_weight=hp_ctrl_cost_weight,\n",
    "    healthy_reward=hp_healthy_reward,\n",
    "    contact_cost_weight=hp_contact_cost_weight,\n",
    "    healthy_z_range=healthy_z,\n",
    "    contact_force_range=contact_force,\n",
    "    render_mode='human'\n",
    ")\n",
    "\"\"\"\n",
    "Questo snippet di codice inizializza un ambiente Gym per il compito \"Ant-v5\" con specifici iperparametri.\n",
    "\n",
    "Parametri:\n",
    "- reset_noise_scale (float): Scala del rumore aggiunto allo stato iniziale durante il reset dell'ambiente.\n",
    "- forward_reward_weight (float): Peso della ricompensa per il movimento in avanti.\n",
    "- ctrl_cost_weight (float): Peso del costo associato alle azioni di controllo.\n",
    "- healthy_reward (float): Ricompensa per mantenere uno stato di salute.\n",
    "- contact_cost_weight (float): Peso del costo associato alle forze di contatto.\n",
    "- healthy_z_range (tuple): Intervallo di valori z considerati sani per l'agente.\n",
    "- contact_force_range (tuple): Intervallo di forze di contatto considerate accettabili.\n",
    "- render_mode (str): Modalità di rendering dell'ambiente, impostata su 'human' per il rendering visivo.\n",
    "\n",
    "Questa configurazione è tipicamente utilizzata negli esperimenti di apprendimento per rinforzo per addestrare e valutare agenti nell'ambiente \"Ant-v5\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env_render.training = False # Setta l'environment in modalità di valutazione\n",
    "env_render.norm_reward = False # Disabilita la normalizzazione della reward. Questo è importante per valutare correttamente il modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 21:07:09.747 Python[9039:415726] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-21 21:07:09.747 Python[9039:415726] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/glfw/__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "# Carica il modello salvato\n",
    "model = PPO.load(\"ppo_Ant_model_PPO18\")\n",
    "\n",
    "# Reset dell'ambiente e inizializzazione delle variabili\n",
    "obs, _ = env_render.reset()\n",
    "done = False\n",
    "\n",
    "# Loop di esecuzione fino a quando l'episodio non è terminato\n",
    "while not done:\n",
    "    # Predice l'azione basata sull'osservazione corrente\n",
    "    action, _ = model.predict(obs)\n",
    "    \n",
    "    # Esegue l'azione nell'ambiente e ottiene la nuova osservazione e lo stato di completamento\n",
    "    obs, _, done, _, _ = env_render.step(action)\n",
    "    \n",
    "    # Renderizza l'ambiente\n",
    "    env_render.render()\n",
    "    \n",
    "    # Pausa per rendere il rendering visibile\n",
    "    time.sleep(0.01)\n",
    "\n",
    "# Chiude l'ambiente\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
