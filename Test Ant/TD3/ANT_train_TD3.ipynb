{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri dell'ambiente\n",
    "hp_reset_noise_scale = 0.04759391555142866  # Scala del rumore quando l'ambiente viene resettato\n",
    "hp_forward_reward_weight = 1.895790973082321  # Peso della ricompensa per il movimento in avanti\n",
    "hp_ctrl_cost_weight = 1.5077149854491863  # Peso del costo di controllo del robot\n",
    "hp_healthy_reward = 2.4052783506228486  # Ricompensa per mantenere lo stato \"sano\"\n",
    "hp_contact_cost_weight = 6.97442897111752e-05  # Peso del costo per i contatti\n",
    "hp_healthy_z_lower = 0.38253131966695203  # Altezza minima considerata \"sana\"\n",
    "hp_healthy_z_upper = 1.1546046945435202  # Altezza massima considerata \"sana\"\n",
    "hp_contact_force_min = -1.254958844476713  # Forza minima di contatto considerata\n",
    "hp_contact_force_max = 0.9972723819502675  # Forza massima di contatto considerata\n",
    "hp_healthy_z_range = (hp_healthy_z_lower, hp_healthy_z_upper)  # Range di altezza \"sana\"\n",
    "hp_contact_force_range = (hp_contact_force_min, hp_contact_force_max)  # Range di forza di contatto\n",
    "\n",
    "# Parametri di ottimizzazione per TD3\n",
    "hp_learning_rate = 0.0001500747188896999  # Learning rate per la rete di attore-critico\n",
    "hp_learning_starts = 5000  # Numero di step prima di iniziare gli aggiornamenti della rete\n",
    "hp_batch_size = 1024  # Dimensione del batch per SGD\n",
    "hp_gamma = 0.943633865948463  # Fattore di sconto per il futuro reward\n",
    "hp_tau = 0.00823476649680571  # Fattore di interpolazione per l'aggiornamento della rete target\n",
    "hp_noise_std = 0.11889226516450727  # Deviazione standard del rumore di esplorazione\n",
    "hp_noise_clip = 0.3182225104935072  # Clipping del rumore per la policy target smoothing\n",
    "hp_policy_delay = 3  # Delay tra gli aggiornamenti della politica rispetto ai Q-network updates\n",
    "hp_train_freq = 10  # Frequenza di aggiornamento della rete (ogni X step di interazione)\n",
    "hp_gradient_steps = 8  # Numero di passi di aggiornamento eseguiti dopo ogni batch\n",
    "\n",
    "# Parametri dell'azione (rumore per esplorazione)\n",
    "hp_action_noise_mean = [0.0] * 8  # Media del rumore gaussiano per l'esplorazione\n",
    "hp_action_noise_sigma = [0.3426192847562814] * 8  # Deviazione standard del rumore gaussiano\n",
    "\n",
    "# Parametri globali\n",
    "hp_num_envs = 6  # Numero di environment paralleli per il training\n",
    "hp_total_timesteps = 3_000_000  # Numero totale di timesteps per l'addestramento\n",
    "hp_episodes_evaluation = 200  # Numero di episodi usati per valutare la policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrà effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crea l'ambiente Ant-v5 con i parametri specificati\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=hp_reset_noise_scale,  # Scala del rumore quando l'ambiente viene resettato\n",
    "                    forward_reward_weight=hp_forward_reward_weight,  # Peso della ricompensa per il movimento in avanti\n",
    "                    ctrl_cost_weight=hp_ctrl_cost_weight,  # Peso del costo di controllo\n",
    "                    healthy_reward=hp_healthy_reward,  # Ricompensa per mantenere lo stato \"sano\"\n",
    "                    contact_cost_weight=hp_contact_cost_weight,  # Peso del costo per i contatti\n",
    "                    healthy_z_range=hp_healthy_z_range,  # Range di altezza considerata \"sana\"\n",
    "                    contact_force_range=hp_contact_force_range,  # Range di forza di contatto considerata\n",
    "                    render_mode='none')  # Nessun rendering durante l'esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato utilizzando SubprocVecEnv per gestire più istanze dell'ambiente in parallelo.\n",
    "NUM_ENVS = hp_num_envs\n",
    "env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "# 2. Normalizziamo osservazioni e ricompense per stabilizzare l'allenamento.\n",
    "# VecNormalize scala le osservazioni e le ricompense e limita i valori delle osservazioni a un range [-10, 10].\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci i parametri del rumore per l'esplorazione\n",
    "n_actions = env.action_space.shape[-1]  # Numero di azioni nell'ambiente\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=hp_action_noise_mean * np.ones(n_actions))  # Rumore gaussiano con media e deviazione standard specificate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza il modello TD3 con la politica MlpPolicy e l'ambiente creato\n",
    "model = TD3(\"MlpPolicy\", env,\n",
    "            learning_rate=hp_learning_rate,  # Imposta il learning rate\n",
    "            buffer_size=50000,  # Dimensione del buffer di replay\n",
    "            learning_starts=hp_learning_starts,  # Numero di step prima di iniziare l'aggiornamento della rete\n",
    "            batch_size=hp_batch_size,  # Dimensione del batch per l'aggiornamento\n",
    "            gamma=hp_gamma,  # Fattore di sconto per il futuro reward\n",
    "            tau=hp_tau,  # Fattore di interpolazione per l'aggiornamento della rete target\n",
    "            action_noise=action_noise,  # Rumore per l'esplorazione\n",
    "            policy_delay=hp_policy_delay,  # Delay tra gli aggiornamenti della politica rispetto ai Q-network updates\n",
    "            train_freq=hp_train_freq,  # Frequenza di aggiornamento della rete\n",
    "            gradient_steps=hp_gradient_steps,  # Numero di passi di aggiornamento eseguiti dopo ogni batch\n",
    "            seed=42,  # Seed per la riproducibilità\n",
    "            verbose=0,  # Livello di verbosità\n",
    "            tensorboard_log=\"./td3_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "            device='mps')  # Dispositivo su cui eseguire il training (es. CPU, GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Crea un ambiente vettorializzato per la valutazione utilizzando SubprocVecEnv per gestire più istanze dell'ambiente in parallelo\n",
    "eval_env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "# Normalizza osservazioni e ricompense per stabilizzare la valutazione\n",
    "# VecNormalize scala le osservazioni e le ricompense e limita i valori delle osservazioni a un range [-10, 10]\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un callback per la valutazione del modello durante l'allenamento\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,  # Ambiente di valutazione\n",
    "    best_model_save_path=\"./logs/best_model\",  # Percorso per salvare il miglior modello\n",
    "    log_path=\"./logs/\",  # Percorso per salvare i log\n",
    "    eval_freq=50000,  # Frequenza di valutazione (ogni 50000 timesteps)\n",
    "    deterministic=True,  # Esegui la valutazione in modo deterministico\n",
    "    render=False  # Non mostrare il rendering durante la valutazione\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alleniamo il modello TD3\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = hp_total_timesteps  # Numero totale di timesteps per l'addestramento\n",
    "model.learn(total_timesteps=total_timesteps, callback=eval_callback)  # Avvia l'allenamento del modello con il callback di valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva il modello TD3 addestrato su disco\n",
    "model.save(\"td3_Ant_model\")\n",
    "\n",
    "# Salva lo stato dell'ambiente vettorializzato normalizzato su disco\n",
    "env.save(\"vecnormalize_Ant.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_policy(env, episodes=hp_episodes_evaluation):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []  # Lista per memorizzare le ricompense totali di ogni episodio\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Resetta l'ambiente e ottiene l'osservazione iniziale\n",
    "        done = [False] * env.num_envs  # Stato di completamento per ogni ambiente\n",
    "        episode_rewards = np.zeros(env.num_envs)  # Ricompense accumulate per ogni ambiente\n",
    "        while not all(done):\n",
    "            actions = [env.action_space.sample() for _ in range(env.num_envs)]  # Genera azioni casuali per ogni ambiente\n",
    "            obs, rewards, done, infos = env.step(actions)  # Esegue le azioni e ottiene le nuove osservazioni e ricompense\n",
    "            episode_rewards += rewards  # Aggiorna le ricompense accumulate\n",
    "        total_rewards.extend(episode_rewards)  # Aggiunge le ricompense dell'episodio alla lista totale\n",
    "    mean_reward_random = np.mean(total_rewards)  # Calcola la ricompensa media\n",
    "    # std_reward_random = np.std(total_rewards)  # Calcola la deviazione standard delle ricompense (commentato)\n",
    "    return mean_reward_random  # Ritorna la ricompensa media"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
